{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2fb306-238e-4022-94ac-1cf8afd577ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from hungry_geese.utils import read_json_lines, read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c88c18-5738-403f-b943-f3b6b8be4a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41426/41426 [00:00<00:00, 48672.39it/s]\n"
     ]
    }
   ],
   "source": [
    "folder = Path('/home/isaiah/data/alphagoose_pretrain_data_with_mmr_1100/')\n",
    "all_files = list(folder.glob('*.ljson'))\n",
    "\n",
    "subfolder_size = 10_000\n",
    "for i, f in enumerate(tqdm.tqdm(all_files)):\n",
    "    subfolder = folder / str(i // subfolder_size)\n",
    "    if i % subfolder_size == 0:\n",
    "        subfolder.mkdir()\n",
    "    #print(f, subfolder / f.name)\n",
    "    f.rename(subfolder / f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37acb1a1-0810-4e5e-b5dd-c19d8c2de597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/isaiah/data/alphagoose_pretrain_data_with_mmr_1100/21173114.ljson'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([str(s) for s in all_files][:100], dtype=np.string_)[0].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3076fe4-2552-4db3-913f-ff979d238699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111200/111200 [02:16<00:00, 811.92it/s] \n"
     ]
    }
   ],
   "source": [
    "all_files = list(Path('/home/isaiah/GitHub/Kaggle/Hungry_Geese/episode_scraping/alphagoose_pretrain_data_with_mmr_1050/').glob('*.ljson'))\n",
    "mmrs = []\n",
    "for filename in tqdm.tqdm(all_files):\n",
    "    for a in read_json_lines(filename, 0):\n",
    "        mmrs.append(a['mmr'])\n",
    "mmrs_orig = mmrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff7d91-b0df-4a31-b45d-c797444b4c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 15997/450100 [01:10<17:25, 415.24it/s]  "
     ]
    }
   ],
   "source": [
    "all_files = list(Path('/home/isaiah/GitHub/Kaggle/Hungry_Geese/episode_scraping/episodes/').glob('*.json'))\n",
    "all_file_contents = []\n",
    "\n",
    "for filename in tqdm.tqdm(all_files):\n",
    "    all_file_contents.append(read_json(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c5a8f6d-ef9a-4851-95cc-9351d5abcf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7831440227809239"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([mmr_to_importance(m) for m in mmrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0fbaf2-3a77-4787-8c3b-38ae56daf994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mmr_to_importance(mmr: float) -> float:\n",
    "    mmr_normalized = (mmr - 1150) / 25\n",
    "    return 2 ** mmr_normalized\n",
    "\n",
    "fake_mmrs = np.linspace(1050, 1300, 1000)\n",
    "plt.plot(fake_mmrs, [mmr_to_importance(m) for m in fake_mmrs])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(mmrs, weights=[mmr_to_importance(m) for m in mmrs], density=True, bins=20)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d47dcf70-c4fa-4cc0-b1b2-b46763a0768b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgNklEQVR4nO3de3wV5Z3H8c8vdxISkpBAgCQQlIsCohARi1q7Foul1bbaWistba10r+1au63W7vay69be17qtLlqrtYqtVFu72qqlVWwFNEGQ+8VwSSAkBxIScr+cZ//IgY0xISHnnMyZ5Pt+vfLKnDmTzO/JA99MZp6Zx5xziIiI/8R5XYCIiAyOAlxExKcU4CIiPqUAFxHxKQW4iIhPJQzlznJyctyUKVOGcpciIr5XWlp61DmX23P9kAb4lClTKCkpGcpdioj4npkd6G29TqGIiPiUAlxExKcU4CIiPqUAFxHxKQW4iIhP9RvgZvagmVWb2dYe6//JzHaZ2TYz+070ShQRkd4M5Aj8IWBJ9xVm9i7gGuA859ws4HuRL01ERE6n3wB3zq0Fanqs/jvgLudca2ib6ijUJiLie42tHXzzd9vZf7Qx4t97sOfApwOXmtkGM3vJzC7sa0MzW2FmJWZWEggEBrk7ERF/em7bER786z4CDa0R/96DDfAEIAtYCPwL8Cszs942dM6tdM4VO+eKc3PfdieoiMiwtrq0goLsURRPzor49x5sgFcAT7ourwJBICdyZYmI+F9FbROvvHmM6+YV0McxblgGG+C/Af4GwMymA0nA0QjVJCIyLPy69BAA186fFJXv3+/DrMxsFXA5kGNmFcDXgAeBB0NDC9uA5U6Ta4qInBIMOlZvLOcdZ40lPys1KvvoN8Cdczf08dayCNciIjJsvLa/hvKaZr6weHrU9qE7MUVEouCJ0gpGJyfwnll5UduHAlxEJMIaWzt4dkslS+dMIDUpetMuKMBFRCLs2S2VNLV18uHi/KjuRwEuIhJhq0srKMpJY34Uxn53pwAXEYmgg8ea2LCvhuvm50dl7Hd3CnARkQhavbECM/jgBdEZ+92dAlxEJEI6g45fl1Zwydk5TMwcFfX9KcBFRCJk7e4Ah443c8OCwiHZnwJcRCRCHnv1IDmjk3j3OeOHZH8KcBGRCDhS18KfdlZz3fwCkhKGJloV4CIiEfBESTmdQccNCwqGbJ8KcBGRMHUGHY+/Vs4lZ+cweWzakO1XAS4iEqa1e4b24uVJCnARkTCt2nCQsWlJLD53aC5enqQAFxEJQ1V9C2t2VnNdcf6QXbw8SQEuIhKGUxcvLxza0ycwgAA3swfNrDo0+07P975oZs7MNB+miIw4Jy9eLjp7LFNyhu7i5UkDOQJ/CFjSc6WZFQCLgYMRrklExBde3FVNRW0zH1sw2ZP99xvgzrm1QE0vb/0Q+BKguTBFZER6eN0B8jJSuHLW0F68PGlQ58DN7GrgkHNu8wC2XWFmJWZWEggEBrM7EZGY82aggbW7A9x4USGJ8d5cTjzjvZpZKnAH8G8D2d45t9I5V+ycK87NzT3T3YmIxKRH1h0gMd746BCP/e5uML82zgKKgM1mth/IBzaaWfRm7hQRiSENrR2sLq1g6ZwJ5KYne1bHGc+26ZzbAow7+ToU4sXOuaMRrEtEJGY9tbGChtYOlr9jiqd1DGQY4SpgHTDDzCrM7KbolyUiEpucczy87gDn5Y/h/IJMT2vp9wjcOXdDP+9PiVg1IiIx7pU3j7G3uoHvf3hu1Oe87I/uxBQROQMPvbKf7LQklp43wetSFOAiIgNVXtPEmh1V3LCggJTEeK/LUYCLiAzUw6/sx8y48SJv7rzsSQEuIjIA9S3tPP5aOUvnTBiSGecHQgEuIjIAv3y1nIbWDj5zaZHXpZyiABcR6UdHZ5Cf/XUfC4qyOS8/0+tyTlGAi4j049mtRzhc18LNl071upS3UICLiJyGc44HXi5jak4aV8wc1/8XDCEFuIjIaby6r4Y3Kur49CVFxMV5e+NOTwpwEZHTuP/lfWSlJnLtvHyvS3kbBbiISB/KAg2s2VnFxxdOZlSS9zfu9KQAFxHpwwN/2UdiXBzLLo6NG3d6UoCLiPSiur6F1SUVXDs/n3HpKV6X0ysFuIhILx74yz46gkH+9p2xNXSwOwW4iEgPx5vaeHT9Ad4/dyKTx6Z5XU6fFOAiIj08/MoBGts6+bvLz/K6lNMayIw8D5pZtZlt7bbuu2a208zeMLOnzCwzqlWKiAyRxtYOfvbKPt59zjhm5mV4Xc5pDeQI/CFgSY91LwCznXPnAbuB2yNcl4iIJ1a9epDjTe38/bvO9rqUfvUb4M65tUBNj3XPO+c6Qi/X0zUzvYiIr7V2dHL/y2VcPHUs8wqzvC6nX5E4B/5p4Pd9vWlmK8ysxMxKAoFABHYnIhIdT248RFV9K3//rtg+931SWAFuZncAHcCjfW3jnFvpnCt2zhXn5uaGszsRkahp7wxy74tvcl7+GC45O8frcgZk0AFuZsuB9wE3Oudc5EoSERl6T208xMGaJj73N9M8n21+oBIG80VmtgT4MvBO51xTZEsSERla7Z1B7vnzHs7LH8MV58TWI2NPZyDDCFcB64AZZlZhZjcB/w2kAy+Y2SYzuy/KdYqIRM2vSysor2nmn9/tn6NvGMARuHPuhl5W/zQKtYiIDLm2jiD3/GkvcwsyedcM/xx9g+7EFJERbnVpBYeO++/oGxTgIjKCtXUE+fGf93J+QSaXT/ffKDkFuIiMWL8qKefQ8WZuWTzdd0ffoAAXkRGqtaOTn/x5L/MKM7lsmj/GffekABeREekX6w9yuK6FLyye4cujb1CAi8gIdKKlnR//eS+Lzh7LJT49+gYFuIiMQPevLaOmsY0vL5npdSlhUYCLyIgSONHKA3/Zx9I5EzgvP9PrcsKiABeREeWeP+2htSPIrVdO97qUsCnARWTEOHisicc2HOT6CwuYmjva63LCpgAXkRHj+y/sIiHe+PwV07wuJSIU4CIyImw7XMdvNx3mU4uKGJ+R4nU5EaEAF5FhzznHnc/sIDM1kb99pz9m2xkIBbiIDHt/3FHNK28e45Z3T2fMqESvy4kYBbiIDGttHUH+89kdnJWbxscuKvS6nIhSgIvIsPbI+gPsO9rIV5eeS2L88Iq8gczI86CZVZvZ1m7rss3sBTPbE/qcFd0yRUTOXG1jG3f/cTeXTsvh8hn+e1xsfwby6+ghYEmPdbcBa5xz04A1odciIjHl7jV7aGjt4KtLz/XtA6tOp98Ad86tBWp6rL4GeDi0/DDwgciWJSISnr3VDTyy/gA3LChkRl661+VExWBPCI13zlUChD77ayI5ERnWnHP8+/9uJzUxnlsW+/+W+b5E/Yy+ma0wsxIzKwkEAtHenYgIz2+v4qXdAf558XRyRid7XU7UDDbAq8xsAkDoc3VfGzrnVjrnip1zxbm5w+8igojElua2Tr75u+3MzEtn+cWTvS4nqgYb4E8Dy0PLy4HfRqYcEZHw/PjPezl0vJlvXD2LhGE2bLCngQwjXAWsA2aYWYWZ3QTcBSw2sz3A4tBrERFP7TvayMq1ZXzwgklcNHWs1+VEXUJ/GzjnbujjrSsiXIuIyKA55/ja09tITojj9vf6e6adgRref1+IyIjx3LYq1u4OcMvi6YxLHx5PG+yPAlxEfK+xtYNv/m4bM/PS+cQwv3DZnQJcRHzve8/vorK+hTs/OHvYX7jsbuS0VESGpU3lx3nolf0su2gy8ydne13OkFKAi4hvtXcGue3XbzA+PYUvLZnhdTlDrt9RKCIisWrl2jJ2HjnByo/PJz1l+EzUMFA6AhcRX9p3tJG71+xhyaw8rpyV53U5nlCAi4jvOOf4ypNbSE6I4xvXzPK6HM8owEXEdx579SDryo5x21Uzh80M84OhABcRXymvaeLOZ3aw6Oyx3HDh8Jrj8kwpwEXEN4JBx7+s3kycGd+5bi5xccNvlp0zoQAXEd/4+br9rC+r4V/fdw6TMkd5XY7nFOAi4gv7jzZy1x92cvmMXD5SXOB1OTFBAS4iMa8z6PjiE5tJio/jrg+dNywnKB4M3cgjIjHvgZfLKDlQyw8+Mpe8MSN31ElPOgIXkZi2paKO7z2/iyWz8vjgBZO8LiemhBXgZnaLmW0zs61mtsrM9KtRRCKmqa2Dzz/+OmPTkrnr2jk6ddLDoAPczCYBnwOKnXOzgXjgo5EqTETkm7/bzr5jjfzw+vPJTE3yupyYE+4plARglJklAKnA4fBLEhGB32+p5PHXyvm7d57FxWcN//ktB2PQAe6cOwR8DzgIVAJ1zrnnI1WYiIxch483c9uTW5ibP4ZbFk/3upyYFc4plCzgGqAImAikmdmyXrZbYWYlZlYSCAQGX6mIjAidQccXfrWJjs4gd3/0AhJH0Aw7Zyqcn8y7gX3OuYBzrh14EnhHz42ccyudc8XOueLc3NwwdiciI8F//XE368tq+MY1s5mSk+Z1OTEtnAA/CCw0s1TrujR8BbAjMmWJyEj04q5q7vnTXj5SnM918/O9LifmhXMOfAOwGtgIbAl9r5URqktERphDx5u55ZebmJmXzjevme11Ob4Q1p2YzrmvAV+LUC0iMkK1dQT5h0c30t7puHfZfFIS470uyRd0K72IeO5bv9/BpvLj3HvjPIp03nvAdHlXRDz1zBuV/Oyv+/nUoilcNWeC1+X4igJcRDyzo7KeLz6xmXmFmdx+1Tlel+M7CnAR8URNYxs3/7yEjFEJ3LdsPkkJiqMzpXPgIjLk2ju7LlpWn2jlV5+9mHEjeGLicOhXnogMuTuf2cG6smN864NzOL8g0+tyfEsBLiJD6levlfPQK/v5zCVFXKubdcKiABeRIfPqvhq++putXDoth9uumul1Ob6nABeRIVEWaGDFIyXkZ4/inhsuIEEPqQqbfoIiEnXHGlr51EOvEW/GQ59coMkZIkSjUEQkqlraO1nxSClH6lpYtWIhhWNTvS5p2FCAi0jUBIOOW5/YTOmBWn5y4zzmFWZ5XdKwolMoIhI1335uJ8+8UcntV83kvbpNPuIU4CISFfevLeN/Xipj2cJCVlw21etyhiUFuIhE3BMl5dz57A6WzpnAN66eTdecLxJpCnARiagXtldx25NbuHRaDj+4fi7xcQrvaFGAi0jErC87xj88tpHZk8Zw37L5JCdoYoZoCivAzSzTzFab2U4z22FmF0eqMBHxly0Vddz8cAmF2ak89MkLSUvWILdoC/cnfDfwB+fcdWaWBGiAp8gItO1wHct+uoExqYn8/NMLyErTjTpDYdABbmYZwGXAJwGcc21AW2TKEhG/2HmknmUPbCAtKZ5VNy9kYuYor0saMcI5hTIVCAA/M7PXzewBM3vbZHZmtsLMSsysJBAIhLE7EYk1e6pOcOP9G0hOiGfVioUUZOuP8KEUToAnAPOAe51zFwCNwG09N3LOrXTOFTvninNzc8PYnYjEkjcDDdxw/wbi44zHbr6IyWM1GfFQCyfAK4AK59yG0OvVdAW6iAxze6sb+Nj96wF47OaFTM0d7XFFI9OgA9w5dwQoN7MZoVVXANsjUpWIxKzth+u5/n/W0RmEx26+iLPHKby9Eu4olH8CHg2NQCkDPhV+SSISq14/WMvyB18lLTmBRz9zkY68PRZWgDvnNgHFkSlFRGLZ+rJj3PTQa+SkJ/OLmy7SBcsYoJH2ItKvF3dV89lHSinITuXRz1zEeM0iHxMU4CJyWr/ddIgvPrGZ6ePT+fmnFzB2dLLXJUmIAlxEeuWc4/6Xy/jPZ3dyUVE2Kz9RzJhRiV6XJd0owEXkbYJBx388s4MH/7qPpXMm8P2PzCUlUQ+mijUKcBF5i5b2Tm59YjPPvFHJpxZN4V+XnkucHgkbkxTgInJKbWMbf/uLUjbsq+Er753JzZdO1WQMMUwBLiIA7K0+wU0Pl1BZ18LdHz2fa86f5HVJ0g8FuIjw4q5q/umx10lOjOfxFQs1e7xPKMBFRjDnHA/+dT93PrOdGXkZPLC8mEl6HKxvKMBFRqjWjk6+/vQ2Vr1azntmjecHHzlfs+j4jHpLZAQ6dLyZv390I5vLj/MP7zqLWxfP0EgTH1KAi4wwL+8J8LlVr9Pe6bhv2TyWzJ7gdUkySApwkREiGHT85MW9fP+F3Uwfl869y+bpaYI+pwAXGQGON7Vx6682s2ZnNdecP5FvfWgOqUn67+936kGRYW7dm8e45ZebONbYyjeunsUnLp6sm3OGCQW4yDDV3hnk7j/u4ccv7mXK2DSe/MQi5uSP8bosiaCwA9zM4oES4JBz7n3hlyQi4SqvaeJzj7/O6weP8+H5+Xz96lkaIjgMRaJHPw/sADIi8L1EJAzOOZ56/RBf++02AH50wwVcPXeix1VJtIQV4GaWDywF7gS+EJGKRGRQqk+08JUnt/LHHVUUT87ih9efr2nPhrlwj8D/C/gSkN7XBma2AlgBUFhYGObuRKQn5xxPbz7M157eRnNbJ19deg6fWlREvG7MGfYGHeBm9j6g2jlXamaX97Wdc24lsBKguLjYDXZ/IvJ2gROtfPU3W3huWxXnF2TyvQ/P5exxGts9UoRzBL4IuNrM3gukABlm9gvn3LLIlCYifXHO8URpBd96dgeNrZ3cdlXXs7t11D2yDDrAnXO3A7cDhI7Av6jwFom+vdUN3PHUFjbsq6F4chbf+tAcpo3v8yymDGMaVyTiEy3tnfzkz3u596U3GZUYz7c+NIfriwv0EKoRLCIB7px7EXgxEt9LRN7upd0Bvv70NvYdbeQD50/kjqXnkpue7HVZ4jEdgYvEsLJAA//xzA7+tLOaKWNTeeSmBVw6LdfrsiRGKMBFYlB9Szv3rNnDQ6/sJzkhntuvmsknF00hOSHe69IkhijARWJIZ9DxREk5331uFzVNbXxkfgFffM8MnS6RXinARWKAc44Xtlfx3ed2sae6gQunZPHw+xcwe5IePiV9U4CLeGxD2TG+/YedbDx4nKm5adx74zyWzM7TI1+lXwpwEY9sP1zPd57byYu7AuRlpHDXh+Zw3fx8EuLjvC5NfEIBLjLEth6q40dr9vD89irGjErk9qtmsvwdU0hJ1AVKOTMKcJEhsqn8OPes2cOandWkpyTw+Sum8elFRYxJTfS6NPEpBbhIlJUeqOFHa/by0u4AY0Ylcuvi6SxfNIWMFAW3hEcBLhIFncGuUSUPvFxGyYFastOS+PKSmXz84smM1sw4EiH6lyQSQc1tnazeWMFPXy5j/7Em8rNG8W/vO5frLyzQlGYScfoXJRIB1fUt/GL9AR5Zf4Dapnbm5o/hxx+bx3tmjdeoEokaBbjIIDnn2LCvhkfWH+C5rUfodI53nzOemy+dyoVTsjSOW6JOAS5yhupb2nlq4yF+sf4Ae6obGDMqkU++Ywo3LpxMUU6a1+XJCKIAFxkA5xxvVNTxy5JyfvP6IZraOpmbP4bvXnce7587UWO4xRMKcJHTqD7RwlMbD7G6tII91Q0kJ8Rx9dyJLFs4mbkFmV6XJyNcOJMaFwA/B/KAILDSOXd3pAoT8UprRydrdlSzurSCl3YH6Aw65oemLlt63gSN35aYEc4ReAdwq3Nuo5mlA6Vm9oJzbnuEahMZMh2dQdaX1fC7zYf5w7Yj1DW3k5eRwmcvm8q18/M5K1czvUvsCWdS40qgMrR8wsx2AJMABbj4QjDoKDlQy/++cZhnt1RytKGN0ckJXHnueK4+fyKXTsvVLO8S0yJyDtzMpgAXABt6eW8FsAKgsLAwErsTGbSOziClB2p5blsVz26p5Eh9CymJcVwxczzvnzuBy2eM0wVJ8Y2wA9zMRgO/Bv7ZOVff833n3EpgJUBxcbELd38iZ6q5rZOX9wR4fnsVa3ZUUdvUTlJ8HJdNz+H2987kinPG6/Z28aWw/tWaWSJd4f2oc+7JyJQkEr7qEy28uCvAC9ureHlPgJb2IOkpCVwxcxxXzsrjsum5Cm3xvXBGoRjwU2CHc+4HkStJ5My1dQTZeLCWl3YHeGlXgO2VXX8MThyTwvXFBVw5K48FRdkk6rZ2GUbCOQRZBHwc2GJmm0LrvuKcezbsqkQGoLymqSuwdwd4Ze9RGts6SYgz5k/O4ktLZnDZtFxmTczQLe0ybIUzCuUvgP5nyJCpqG1ifVkN68uOsb7sGBW1zQDkZ43iAxdM4p3Tc7n4rLGka5y2jBA6CSgx69DxZta/2RXW67oFdlZqIgunjuUzlxRx6fRcpuak6ShbRiQFuMSEto4g2w7XUXqgltcPHmfjwVoq61oAyExN5KKibD5zSRELzxrL9HHpxGl8togCXLxRXd/CxoO1bDx4nNIDtWw5VEdbRxCASZmjKJ6SzbzCTBZOHcuM8Qpskd4owCWqnHNU1bey5VAdW0MfWw7VUX2iFYCk+DhmT8rgEwsnM39yFvMmZzE+I8XjqkX8QQEuERMMOipqm9lxpP5UUG89VM/Rhq6wjjM4K3c0i87OYfakMVxQmMmsiRkkJ+jOR5HBUIDLGXPOcbShjV1HTrCr6gS7jtSzq6qBPVUnaGrrBCA+zpg2bjSXz8hl9sQM5uSP4ZwJGaQm6Z+cSKTof5P0KRh0VNa3UBZoYN/RRt6sbmB3VQO7qk5Q09h2aruc0UnMyEvn+gsLmJmXzvTx6ZwzIUPPFBGJMgW4UN/STlmg8VRQlwUaeTPQwP5jjbS0B09tl5YUz7Tx6Vx57nimj0/vCuu8dHJGJ3tYvcjIpQAfAdo7gxw+3kx5TTMHa5oor22ivKaJ8tpmymua3nI0HR9nFGSNYmruaC45O4ei3DSm5ozmrNw0ctOTNd5aJIYowIeBxtYOKutaOFLXQmVdM5V1LaGAbqK8ppnKumaC3Z4DmRBn5GeNoiA7lffMymPy2FSm5qQxNXc0hdmpJCXoeSEifqAAj2HOOeqbOwg0tFBZF/o43sKR+uZTy5V1zdS3dLzta8elJ1OQncqComwKskaRn51KYXYqBdmp5GWkaKICkWFAAe6B5rZOAidaCTS0EDjRRqChtev1yY+GVo6Glts6g2/7+pzRSUwYM4rCsaksnJpN3phRTBiTEvoYxbiMZF1AFBkBFOBhau8MUtvURm1je+hzGzVNbRxvaqemset1bVMbNU3tHG9q41hDGw2tbz9iNoOxacnkjE4iNz351Dnn3NHJ5KYnk5eRwsTMrnDWuGkRAQU4zjla2oOcaGmnvqWduuYO6lvaqW9up76lI/S5nROnljuoa2qjtqmd2sY2TvQSxielJcWTlZZEVmoSWWlJFI1NJSstiXHpKaeC+uRHdmoSCXpWtYicAV8GeDDoaGrvpKm1g4bWDpraOmls7aCxrYPG1pPL3dd10NTaeWrbrrD+/3Bu7zz9TG9J8XFkjEokY1QC6SmJjElNoignjczUJLLTusI5OzWJrNTEruW0JDJTE3WkLCJR5YsA/9GaPTy5seJUKJ+8228gkhPiSEtOIC05nrSkBFJDR8WFY9PISEnoCuaUrnDu+pxIekrCW9bpfLKIxKJw58RcAtwNxAMPOOfuikhVPYzPSGZOfiajk+NJTUroCuSk+LcEc9dy9/VdYa0ptERkuApnTsx44MfAYqACeM3MnnbObY9UcSddf2Eh119YGOlvKyLia+Ecni4A9jrnypxzbcDjwDWRKUtERPoTToBPAsq7va4IrXsLM1thZiVmVhIIBMLYnYiIdBdOgPd2K9/bhnM451Y654qdc8W5ublh7E5ERLoLJ8ArgIJur/OBw+GVIyIiAxVOgL8GTDOzIjNLAj4KPB2ZskREpD+DHoXinOsws38EnqNrGOGDzrltEatMREROK6xx4M65Z4FnI1SLiIicAd3lIiLiU+bc6Z8DEtGdmQWAA4P88hzgaATL8ZLaEnuGSztAbYlV4bRlsnPubcP4hjTAw2FmJc65Yq/riAS1JfYMl3aA2hKrotEWnUIREfEpBbiIiE/5KcBXel1ABKktsWe4tAPUllgV8bb45hy4iIi8lZ+OwEVEpBsFuIiIT3ka4Gb2oJlVm9nWbuuyzewFM9sT+pzV7b3bzWyvme0ys/d0Wz/fzLaE3vuRmfX2pMSYaIeZTTGzZjPbFPq4L1bacZq2fNjMtplZ0MyKe2wfk30SqmHAbYnlfumjHd81s51m9oaZPWVmmd3e81uf9NqWWO6T07Tl30Pt2GRmz5vZxG7vRb5fnHOefQCXAfOArd3WfQe4LbR8G/Dt0PK5wGYgGSgC3gTiQ++9ClxM1yNufw9cFcPtmNJ9ux7fx9N2nKYt5wAzgBeB4m7rY7ZPBtGWmO2XPtpxJZAQWv62H/6fDKItMdsnp2lLRrflzwH3RbNfPD0Cd86tBWp6rL4GeDi0/DDwgW7rH3fOtTrn9gF7gQVmNoGuH9o61/XT+Hm3rxkSZ9iOXsVCO6D3tjjndjjndvWyecz2CZxxW3oVC23pox3PO+c6Qi/X0/U4Z/Bnn/TVll7FeFvqu71M4//nSIhKv8TiOfDxzrlKgNDncaH1fc0ANCm03HO91/pqB0CRmb1uZi+Z2aWhdbHajtPxW5/0x6/98mm6jtzA/33SvS3gwz4xszvNrBy4Efi30Oqo9EssBnhf+poBaEAzA8WQSqDQOXcB8AXgMTPLwH/tgOHTJ+DTfjGzO4AO4NGTq3rZzBd90ktbfNknzrk7nHMFdLXjH0Oro9IvsRjgVaE/K07+qVQdWt/XDEAVvPVPrliZGajXdoT+hDoWWi6l61zYdGK3Hafjtz7pkx/7xcyWA+8Dbgz9+Q0+7ZPe2uLHPunhMeDa0HJU+iUWA/xpYHloeTnw227rP2pmyWZWBEwDXg2dnjhhZgtDV28/0e1rvNRrO8ws18ziQ8tT6WpHWQy343T81id98lu/mNkS4MvA1c65pm5v+a5P+mqL3/oEwMymdXt5NbAztBydfhnqK7c9ruKuouvPpHa6fhPdBIwF1gB7Qp+zu21/B12/hXfR7UotUAxsDb3334TuMI3FdtD1G3kbXVekNwLvj5V2nKYtHwwttwJVwHOx3idn2pZY7pc+2rGXrnOqm0If9/m4T3ptSyz3yWna8utQXW8AvwMmRbNfdCu9iIhPxeIpFBERGQAFuIiITynARUR8SgEuIuJTCnAREZ9SgIuI+JQCXETEp/4PjK5UGDboBfMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def mmr_to_importance(mmr: float) -> float:\n",
    "    mmr_normalized = (mmr - 1100) / 50.\n",
    "    return 2 ** mmr_normalized\n",
    "\n",
    "plt.plot(np.linspace(1000, 1300, 1000), [mmr_to_importance(m) for m in np.linspace(1000, 1300, 1000)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "danish-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 18867 episodes into 3107 samples from 9433 train episodes and 3418 samples from 9434 test episodes.\n",
      "Saving results to /Windows/Users/isaia/Documents/GitHub/Kaggle/Hungry_Geese/runs/supervised_pretraining/active/supervised_pretraining_combined_gradient_obs_full_rank_on_death_none_6_blocks_128_dims_v14.TEMP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #0 train: 100%|██████████| 4/4 [00:01<00:00,  2.24it/s]\n",
      "Epoch #0 test: 100%|██████████| 4/4 [00:01<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from hungry_geese.config import N_ROWS, N_COLS\n",
    "from hungry_geese.nns import models, conv_blocks\n",
    "from hungry_geese.nns.misc import Simple1x1Conv\n",
    "import hungry_geese.env.goose_env as ge\n",
    "from hungry_geese.training.alphagoose import alphagoose_data\n",
    "from hungry_geese.training.alphagoose.supervised_pretraining import SupervisedPretraining\n",
    "from hungry_geese.utils import format_experiment_name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "\n",
    "    obs_type = ge.ObsType.COMBINED_GRADIENT_OBS_FULL\n",
    "    n_heads = 4\n",
    "    n_channels = n_heads * 32\n",
    "    activation = nn.GELU\n",
    "    normalize = True\n",
    "    use_preprocessing = True\n",
    "    model_kwargs = dict(\n",
    "        preprocessing_layer=nn.Sequential(\n",
    "            Simple1x1Conv(\n",
    "                obs_type.get_obs_spec()[-3],\n",
    "                n_channels\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        ) if use_preprocessing else None,\n",
    "        base_model=nn.Sequential(\n",
    "            conv_blocks.BasicAttentionBlock(\n",
    "                in_channels=n_channels if use_preprocessing else obs_type.get_obs_spec()[-3],\n",
    "                out_channels=n_channels,\n",
    "                mhsa_heads=n_heads,\n",
    "                activation=activation,\n",
    "                normalize=normalize\n",
    "            ),\n",
    "            conv_blocks.BasicAttentionBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                mhsa_heads=n_heads,\n",
    "                activation=activation,\n",
    "                normalize=normalize\n",
    "            ),\n",
    "            conv_blocks.BasicAttentionBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                mhsa_heads=n_heads,\n",
    "                activation=activation,\n",
    "                normalize=normalize\n",
    "            ),\n",
    "            conv_blocks.BasicAttentionBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                mhsa_heads=n_heads,\n",
    "                activation=activation,\n",
    "                normalize=normalize\n",
    "            ),\n",
    "            conv_blocks.BasicAttentionBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                mhsa_heads=n_heads,\n",
    "                activation=activation,\n",
    "                normalize=normalize\n",
    "            ),\n",
    "            conv_blocks.BasicAttentionBlock(\n",
    "                in_channels=n_channels,\n",
    "                out_channels=n_channels,\n",
    "                mhsa_heads=n_heads,\n",
    "                activation=activation,\n",
    "                normalize=normalize\n",
    "            ),\n",
    "            nn.LayerNorm([n_channels, N_ROWS, N_COLS])\n",
    "        ),\n",
    "        base_out_channels=n_channels,\n",
    "        actor_critic_activation=nn.GELU,\n",
    "        n_action_value_layers=2,\n",
    "        cross_normalize_value=True,\n",
    "        use_separate_action_value_heads=True,\n",
    "        # **ge.RewardType.RANK_ON_DEATH.get_recommended_value_activation_scale_shift_dict()\n",
    "    )\n",
    "    model = models.FullConvActorCriticNetwork(**model_kwargs)\n",
    "    \"\"\"\n",
    "    run_dir = 'runs/deep_q/deep_q_head_centered_obs_small_every_step_length_opposite_6_blocks_32_64_128_dims_v1/'\n",
    "    with open(f'{run_dir}/0620/cp.txt', 'r') as f:\n",
    "        serialized_string = f.readline()[2:-1].encode()\n",
    "    state_dict_bytes = base64.b64decode(serialized_string)\n",
    "    loaded_state_dicts = pickle.loads(state_dict_bytes)\n",
    "    model.load_state_dict(loaded_state_dicts)\n",
    "    \"\"\"\n",
    "    model.to(device=DEVICE)\n",
    "    \"\"\"\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=0.05,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \"\"\"\n",
    "    batch_size = 1024\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "    )\n",
    "    # NB: lr_scheduler counts steps in batches, not epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        # Stop reducing LR beyond 5e-4\n",
    "        # milestones=[int(150000 * 512 * i / batch_size) for i in [1., 2.5]],\n",
    "        milestones=[],\n",
    "        gamma=0.1\n",
    "    )\n",
    "\n",
    "    dataset_loc = Path('/home/isaiah/data/TEMP/')\n",
    "    with open(dataset_loc / 'all_saved_episodes.txt', 'r') as f:\n",
    "        all_episodes = [replay_name.rstrip() for replay_name in f.readlines()]\n",
    "    train_episodes, test_episodes = train_test_split(np.array(all_episodes), test_size=0.5)\n",
    "    train_episodes = set(train_episodes)\n",
    "    test_episodes = set(test_episodes)\n",
    "    train_dataset = alphagoose_data.AlphaGoosePretrainDataset(\n",
    "        dataset_loc,\n",
    "        obs_type,\n",
    "        transform=transforms.Compose([\n",
    "            alphagoose_data.PretrainRandomReflect(obs_type),\n",
    "            alphagoose_data.ChannelShuffle(obs_type),\n",
    "            alphagoose_data.ToTensor()\n",
    "        ]),\n",
    "        include_episode=lambda x: x.stem in train_episodes\n",
    "    )\n",
    "    test_dataset = alphagoose_data.AlphaGoosePretrainDataset(\n",
    "        dataset_loc,\n",
    "        obs_type,\n",
    "        transform=alphagoose_data.ToTensor(),\n",
    "        include_episode=lambda x: x.stem in test_episodes\n",
    "    )\n",
    "    print(f'Split {len(train_episodes) + len(test_episodes)} episodes into '\n",
    "          f'{len(train_dataset)} samples from {len(train_episodes)} train episodes and '\n",
    "          f'{len(test_dataset)} samples from {len(test_episodes)} test episodes.')\n",
    "    dataloader_kwargs = dict(\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=0, **dataloader_kwargs)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, **dataloader_kwargs)\n",
    "\n",
    "    experiment_name = 'supervised_pretraining_' + format_experiment_name(\n",
    "        obs_type,\n",
    "        ge.RewardType.RANK_ON_DEATH,\n",
    "        ge.ActionMasking.NONE,\n",
    "        [n_channels],\n",
    "        model_kwargs.get('block_kwargs', model_kwargs['base_model'][:-1])\n",
    "    ) + '_v14.TEMP'\n",
    "    exp_folder = Path(f'runs/supervised_pretraining/active/{experiment_name}')\n",
    "    train_alg = SupervisedPretraining(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        policy_weight=1.,\n",
    "        value_weight=1.,\n",
    "        entropy_weight=0.1,\n",
    "        device=DEVICE,\n",
    "        use_mixed_precision=True,\n",
    "        exp_folder=exp_folder,\n",
    "        clip_grads=10.,\n",
    "        checkpoint_freq=20.,\n",
    "        checkpoint_render_n_games=5\n",
    "    )\n",
    "    #this_script = Path(__file__).absolute()\n",
    "    #shutil.copy(this_script, train_alg.exp_folder / f'_{this_script.name}')\n",
    "    with open(train_alg.exp_folder / 'train_episodes.txt', 'w') as f:\n",
    "        f.writelines([f'{rn}\\n' for rn in sorted(list(train_episodes), key=lambda x: int(x))])\n",
    "    with open(train_alg.exp_folder / 'test_episodes.txt', 'w') as f:\n",
    "        f.writelines([f'{rn}\\n' for rn in sorted(list(test_episodes), key=lambda x: int(x))])\n",
    "\n",
    "    try:\n",
    "        with torch.autograd.profiler.profile(enabled=True, use_cuda=True) as prof:\n",
    "            train_alg.train(n_epochs=1)\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt: saving model')\n",
    "        train_alg.save(train_alg.exp_folder, finished=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-sister",
   "metadata": {},
   "source": [
    "# 2070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protecting-leather",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::einsum         2.20%      22.533ms        12.13%     124.375ms     423.044us  -49763.219us        -5.53%     125.730ms     427.654us           294  \n",
      "                                 aten::stack         2.03%      20.863ms        11.90%     121.979ms       3.210ms      89.847ms         9.98%     122.721ms       3.229ms            38  \n",
      "                                  aten::set_        10.38%     106.404ms        10.38%     106.404ms       3.254us     109.741ms        12.19%     109.741ms       3.356us         32698  \n",
      "                                    aten::to         1.20%      12.345ms        18.84%     193.146ms     111.323us      11.072ms         1.23%      86.987ms      50.137us          1735  \n",
      "                                aten::matmul         1.08%      11.110ms         7.16%      73.415ms     308.467us       8.206ms         0.91%      71.553ms     300.645us           238  \n",
      "                                BmmBackward0         0.36%       3.668ms         2.86%      29.315ms     232.660us     614.469us         0.07%      53.175ms     422.020us           126  \n",
      "                            aten::layer_norm         0.58%       5.912ms         4.14%      42.473ms     233.365us  -32622.625us        -3.62%      42.688ms     234.548us           182  \n",
      "                                   aten::add         0.91%       9.283ms         1.10%      11.242ms      24.493us      40.441ms         4.49%      40.441ms      88.106us           459  \n",
      "                              RepeatBackward         0.16%       1.641ms         1.19%      12.161ms     337.794us     577.656us         0.06%      37.017ms       1.028ms            36  \n",
      "                     NativeLayerNormBackward         0.40%       4.120ms         1.09%      11.190ms     286.916us     208.250us         0.02%      35.939ms     921.525us            39  \n",
      "              torch::autograd::CopyBackwards         0.57%       5.829ms         2.04%      20.929ms      52.454us       1.403ms         0.16%      33.274ms      83.394us           399  \n",
      "                                aten::repeat         0.82%       8.402ms         2.97%      30.417ms     334.251us       4.411ms         0.49%      28.730ms     315.715us            91  \n",
      "                             SoftmaxBackward         0.02%     208.849us         0.17%       1.780ms      74.173us     117.250us         0.01%      21.988ms     916.177us            24  \n",
      "                                   aten::mul         0.41%       4.220ms         0.50%       5.157ms      24.673us      17.977ms         2.00%      17.977ms      86.014us           209  \n",
      "                               aten::permute         5.39%      55.290ms         5.63%      57.679ms      20.906us      17.565ms         1.95%      17.565ms       6.367us          2759  \n",
      "                               SliceBackward         0.29%       2.960ms         1.97%      20.188ms     112.158us     589.781us         0.07%      16.713ms      92.850us           180  \n",
      "                                ViewBackward         0.67%       6.894ms         2.74%      28.140ms      48.601us       2.037ms         0.23%      13.906ms      24.017us           579  \n",
      "                                  aten::add_         2.23%      22.901ms         2.23%      22.901ms      25.474us      13.880ms         1.54%      13.880ms      15.439us           899  \n",
      "                                 aten::zeros         0.42%       4.346ms         1.47%      15.113ms      39.357us       4.184ms         0.46%      13.380ms      34.843us           384  \n",
      "                 torch::autograd::CopySlices         0.03%     337.827us         0.29%       3.015ms     167.500us     144.188us         0.02%      12.475ms     693.031us            18  \n",
      "                                  MmBackward         0.20%       2.021ms         1.02%      10.471ms     205.322us     543.406us         0.06%      11.503ms     225.541us            51  \n",
      "                            aten::pin_memory         0.63%       6.449ms         1.15%      11.757ms     335.928us       6.522ms         0.72%      11.110ms     317.436us            35  \n",
      "                                  aten::sqrt         0.60%       6.174ms         0.99%      10.156ms      18.808us       6.642ms         0.74%       9.630ms      17.834us           540  \n",
      "                                GeluBackward         0.02%     199.256us         0.07%     740.162us      30.840us     103.438us         0.01%       9.536ms     397.344us            24  \n",
      "                                  aten::mul_         0.79%       8.093ms         0.79%       8.093ms      14.988us       8.561ms         0.95%       8.561ms      15.853us           540  \n",
      "                            aten::zeros_like         0.18%       1.862ms         0.69%       7.057ms      28.006us       3.440ms         0.38%       6.991ms      27.743us           252  \n",
      "                          UnsafeViewBackward         0.64%       6.539ms         1.15%      11.825ms      55.516us     723.250us         0.08%       6.978ms      32.761us           213  \n",
      "                     aten::is_floating_point         0.77%       7.883ms         0.77%       7.883ms       3.343us       6.946ms         0.77%       6.946ms       2.946us          2358  \n",
      "                                  aten::norm         0.55%       5.614ms         0.63%       6.497ms      23.799us       6.669ms         0.74%       6.669ms      24.429us           273  \n",
      "                             PermuteBackward         1.59%      16.332ms         4.07%      41.731ms      51.329us       2.616ms         0.29%       6.582ms       8.096us           813  \n",
      "                                  aten::gelu         0.17%       1.769ms         0.64%       6.554ms      58.518us       2.621ms         0.29%       6.276ms      56.034us           112  \n",
      "                               aten::softmax         0.09%     935.391us         0.67%       6.842ms      61.088us  -12588.688us        -1.40%       6.213ms      55.475us           112  \n",
      "                              aten::addcdiv_         0.35%       3.544ms         0.57%       5.891ms      21.817us       3.867ms         0.43%       6.200ms      22.961us           270  \n",
      "                           AsStridedBackward         0.02%     251.168us         0.15%       1.511ms      83.919us      87.562us         0.01%       6.192ms     344.019us            18  \n",
      "                               IndexBackward         0.02%     220.689us         0.59%       6.099ms     406.612us     198.062us         0.02%       6.116ms     407.760us            15  \n",
      "                              aten::addcmul_         0.34%       3.472ms         0.56%       5.712ms      21.157us       3.816ms         0.42%       6.060ms      22.444us           270  \n",
      "                                 aten::index         0.19%       1.957ms         0.49%       5.006ms      83.436us       2.733ms         0.30%       5.555ms      92.576us            60  \n",
      "                                MulBackward0         0.25%       2.523ms         0.42%       4.311ms      95.790us       2.382ms         0.26%       5.033ms     111.851us            45  \n",
      "                                   aten::div         0.46%       4.685ms         0.51%       5.229ms      18.478us       4.969ms         0.55%       4.969ms      17.559us           283  \n",
      "    aten::_amp_non_finite_check_and_unscale_         0.49%       4.992ms         0.49%       4.992ms      18.490us       4.846ms         0.54%       4.846ms      17.948us           270  \n",
      "                                aten::detach         0.36%       3.688ms         0.51%       5.190ms      13.551us       2.598ms         0.29%       3.891ms      10.160us           383  \n",
      "                                   aten::sum         0.15%       1.514ms         0.39%       3.959ms      41.235us       1.536ms         0.17%       3.740ms      38.960us            96  \n",
      "             torch::autograd::AccumulateGrad         0.59%       6.030ms         1.80%      18.480ms      68.446us       1.022ms         0.11%       2.999ms      11.109us           270  \n",
      "                                 aten::where         0.07%     700.116us         0.28%       2.829ms      34.084us       1.038ms         0.12%       2.673ms      32.207us            83  \n",
      "                     aten::repeat_interleave         0.04%     458.697us         0.26%       2.624ms     187.428us     486.625us         0.05%       2.487ms     177.676us            14  \n",
      "                              SWhereBackward         0.03%     340.589us         0.22%       2.300ms     127.794us     341.062us         0.04%       2.294ms     127.458us            18  \n",
      "                           aten::log_softmax         0.02%     234.809us         0.22%       2.227ms      79.537us     183.750us         0.02%       2.115ms      75.531us            28  \n",
      "                                aten::arange         0.03%     292.858us         0.15%       1.578ms      56.349us       1.161ms         0.13%       1.857ms      66.334us            28  \n",
      "                                    aten::gt         0.02%     241.457us         0.05%     524.555us      37.468us     960.844us         0.11%       1.224ms      87.444us            14  \n",
      "                                AddBackward0         0.18%       1.848ms         0.18%       1.848ms      13.392us       1.175ms         0.13%       1.175ms       8.515us           138  \n",
      "                                    aten::eq         0.07%     695.617us         0.11%       1.127ms      22.531us     714.500us         0.08%       1.064ms      21.271us            50  \n",
      "                                  aten::item         0.04%     419.717us         0.10%     992.953us      14.820us     414.348us         0.05%       1.006ms      15.018us            67  \n",
      "                          UnsqueezeBackward0         0.90%       9.188ms         1.14%      11.668ms      19.065us     995.125us         0.11%     995.125us       1.626us           612  \n",
      "                                 CatBackward         0.13%       1.376ms         0.28%       2.907ms      26.913us     532.750us         0.06%     876.031us       8.111us           108  \n",
      "                                  aten::relu         0.01%     119.059us         0.04%     391.748us      55.964us     163.156us         0.02%     843.250us     120.464us             7  \n",
      "                                   aten::neg         0.05%     464.457us         0.07%     755.896us      22.232us     509.094us         0.06%     745.156us      21.916us            34  \n",
      "                              aten::mse_loss         0.03%     277.579us         0.08%     784.317us      56.023us     261.781us         0.03%     723.906us      51.708us            14  \n",
      "                               ReluBackward0         0.00%      28.129us         0.01%      91.599us      30.533us       9.094us         0.00%     565.219us     188.406us             3  \n",
      "                              aten::nll_loss         0.01%     147.689us         0.06%     606.625us      43.330us     -69.469us        -0.01%     553.844us      39.560us            14  \n",
      "                                  aten::rsub         0.02%     159.716us         0.02%     178.726us      25.532us     548.688us         0.06%     548.688us      78.384us             7  \n",
      "                                   aten::sub         0.05%     489.934us         0.05%     549.914us      19.640us     522.438us         0.06%     522.438us      18.658us            28  \n",
      "                          LogSoftmaxBackward         0.01%      70.849us         0.03%     330.477us      55.080us      61.344us         0.01%     310.781us      51.797us             6  \n",
      "                                MaxBackward0         0.00%      31.340us         0.03%     308.437us     102.812us      30.844us         0.00%     307.375us     102.458us             3  \n",
      "                              aten::randperm         0.01%     137.099us         0.03%     290.138us      72.535us     145.401us         0.02%     290.778us      72.694us             4  \n",
      "                             MseLossBackward         0.00%      33.030us         0.03%     272.187us      90.729us      34.062us         0.00%     272.094us      90.698us             3  \n",
      "                               CloneBackward         0.37%       3.814ms         0.37%       3.814ms      23.546us     263.125us         0.03%     263.125us       1.624us           162  \n",
      "                              ExpandBackward         0.01%     115.689us         0.03%     264.719us      11.030us     104.406us         0.01%     252.312us      10.513us            24  \n",
      "                              aten::linspace         0.01%     130.019us         0.02%     211.708us      15.122us     145.375us         0.02%     214.375us      15.312us            14  \n",
      "                                   aten::max         0.02%     187.020us         0.02%     223.400us      31.914us     201.531us         0.02%     201.531us      28.790us             7  \n",
      "                               MeanBackward0         0.00%      45.050us         0.02%     196.738us      65.579us      66.062us         0.01%     196.250us      65.417us             3  \n",
      "                             NllLossBackward         0.00%      38.670us         0.01%     147.319us      49.106us      30.375us         0.00%     190.750us      63.583us             3  \n",
      "                                   TBackward         0.02%     210.828us         0.05%     490.537us       9.618us     187.125us         0.02%     187.125us       3.669us            51  \n",
      "                                  aten::full         0.01%      97.619us         0.02%     203.327us      40.665us     119.312us         0.01%     165.938us      33.188us             5  \n",
      "                            aten::reciprocal         0.02%     204.669us         0.03%     336.028us      28.002us     104.500us         0.01%     156.719us      13.060us            12  \n",
      "                                    aten::lt         0.01%     110.819us         0.02%     173.788us      28.965us      94.281us         0.01%     148.875us      24.812us             6  \n",
      "                                DivBackward0         0.00%      46.230us         0.01%     131.129us      43.710us      49.812us         0.01%     132.438us      44.146us             3  \n",
      "                                 NegBackward         0.00%      22.450us         0.01%     129.409us      43.136us      23.750us         0.00%     129.000us      43.000us             3  \n",
      "                            aten::is_nonzero         0.00%      20.989us         0.01%     132.879us      44.293us      20.406us         0.00%      98.250us      32.750us             3  \n",
      "                                aten::argmax         0.01%      91.349us         0.01%     105.279us      26.320us      86.125us         0.01%      86.125us      21.531us             4  \n",
      "                               aten::random_         0.01%      79.609us         0.01%      79.609us      19.902us      73.734us         0.01%      73.734us      18.434us             4  \n",
      "                             aten::ones_like         0.00%      24.030us         0.01%      63.840us      21.280us      39.750us         0.00%      63.500us      21.167us             3  \n",
      "                     aten::_amp_update_scale         0.01%      56.349us         0.01%      77.119us      25.706us      63.250us         0.01%      63.250us      21.083us             3  \n",
      "                                SumBackward1         0.00%      24.360us         0.01%      56.950us      18.983us      57.250us         0.01%      57.250us      19.083us             3  \n",
      "                            SqueezeBackward1         0.00%      17.380us         0.00%      46.150us      15.383us      45.844us         0.01%      45.844us      15.281us             3  \n",
      "                          TransposeBackward0         0.00%      16.800us         0.00%      34.110us      11.370us      35.500us         0.00%      35.500us      11.833us             3  \n",
      "                                SubBackward0         0.00%      34.879us         0.00%      34.879us       3.875us      32.781us         0.00%      32.781us       3.642us             9  \n",
      "                     aten::broadcast_tensors         0.00%      26.900us         0.00%      26.900us       3.843us      23.812us         0.00%      23.812us       3.402us             7  \n",
      "                  torch::autograd::GraphRoot         0.00%      10.750us         0.00%      10.750us       3.583us      10.156us         0.00%      10.156us       3.385us             3  \n",
      "                                 aten::empty         5.00%      51.306ms         5.00%      51.306ms       1.377us       0.000us         0.00%       0.000us       0.000us         37268  \n",
      "                                     aten::t         0.11%       1.135ms         0.19%       1.930ms       5.392us       0.000us         0.00%       0.000us       0.000us           358  \n",
      "                               aten::squeeze         0.20%       2.034ms         0.25%       2.520ms       4.071us       0.000us         0.00%       0.000us       0.000us           619  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.025s\n",
      "CUDA time total: 900.187ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", top_level_events_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "solved-university",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 aten::copy_        21.12%     216.566ms        21.12%     216.581ms      89.496us     254.804ms        28.31%     254.804ms     105.291us          2420  \n",
      "                                   aten::bmm         4.15%      42.588ms         8.44%      86.550ms     103.035us     121.139ms        13.46%     225.092ms     267.966us           840  \n",
      "                                aten::einsum         2.20%      22.533ms        12.13%     124.375ms     423.044us  -49763.219us        -5.53%     125.730ms     427.654us           294  \n",
      "                                 aten::stack         2.03%      20.863ms        11.90%     121.979ms       3.210ms      89.847ms         9.98%     122.721ms       3.229ms            38  \n",
      "                                  aten::set_        10.38%     106.404ms        10.38%     106.404ms       3.254us     109.741ms        12.19%     109.741ms       3.356us         32698  \n",
      "                               aten::reshape         1.76%      18.087ms         5.43%      55.644ms      28.203us       7.840ms         0.87%      88.932ms      45.074us          1973  \n",
      "                                    aten::to         1.20%      12.345ms        18.84%     193.146ms     111.323us      11.072ms         1.23%      86.987ms      50.137us          1735  \n",
      "                                 aten::clone         1.31%      13.418ms         3.11%      31.934ms      59.137us       2.781ms         0.31%      85.231ms     157.835us           540  \n",
      "                                aten::matmul         1.08%      11.110ms         7.16%      73.415ms     308.467us       8.206ms         0.91%      71.553ms     300.645us           238  \n",
      "                                   aten::cat         0.86%       8.780ms         6.19%      63.476ms     117.114us   -3764.781us        -0.42%      64.104ms     118.274us           542  \n",
      "                                BmmBackward0         0.36%       3.668ms         2.86%      29.315ms     232.660us     614.469us         0.07%      53.175ms     422.020us           126  \n",
      "                                  aten::_cat         3.50%      35.922ms         3.93%      40.292ms     138.938us      52.639ms         5.85%      52.799ms     182.064us           290  \n",
      "                            aten::contiguous         0.72%       7.362ms         2.20%      22.534ms      50.525us       1.495ms         0.17%      50.123ms     112.383us           446  \n",
      "                            aten::layer_norm         0.58%       5.912ms         4.14%      42.473ms     233.365us  -32622.625us        -3.62%      42.688ms     234.548us           182  \n",
      "                                   aten::add         0.91%       9.283ms         1.10%      11.242ms      24.493us      40.441ms         4.49%      40.441ms      88.106us           459  \n",
      "                              RepeatBackward         0.16%       1.641ms         1.19%      12.161ms     337.794us     577.656us         0.06%      37.017ms       1.028ms            36  \n",
      "                     NativeLayerNormBackward         0.40%       4.120ms         1.09%      11.190ms     286.916us     208.250us         0.02%      35.939ms     921.525us            39  \n",
      "              torch::autograd::CopyBackwards         0.57%       5.829ms         2.04%      20.929ms      52.454us       1.403ms         0.16%      33.274ms      83.394us           399  \n",
      "                                aten::repeat         0.82%       8.402ms         2.97%      30.417ms     334.251us       4.411ms         0.49%      28.730ms     315.715us            91  \n",
      "                     aten::native_layer_norm         0.41%       4.214ms         0.64%       6.555ms      72.031us      28.058ms         3.12%      28.058ms     308.325us            91  \n",
      "            aten::native_layer_norm_backward         0.37%       3.826ms         0.44%       4.522ms     115.939us      25.306ms         2.81%      25.306ms     648.882us            39  \n",
      "                             SoftmaxBackward         0.02%     208.849us         0.17%       1.780ms      74.173us     117.250us         0.01%      21.988ms     916.177us            24  \n",
      "                                    aten::mm         1.70%      17.390ms         1.79%      18.385ms      84.333us      21.970ms         2.44%      21.970ms     100.779us           218  \n",
      "                aten::_softmax_backward_data         0.07%     704.022us         0.15%       1.571ms      65.471us      10.021ms         1.11%      21.871ms     911.292us            24  \n",
      "                                   aten::mul         0.41%       4.220ms         0.50%       5.157ms      24.673us      17.977ms         2.00%      17.977ms      86.014us           209  \n",
      "                               aten::permute         5.39%      55.290ms         5.63%      57.679ms      20.906us      17.565ms         1.95%      17.565ms       6.367us          2759  \n",
      "                               SliceBackward         0.29%       2.960ms         1.97%      20.188ms     112.158us     589.781us         0.07%      16.713ms      92.850us           180  \n",
      "                        aten::slice_backward         0.26%       2.623ms         1.68%      17.228ms      95.713us     868.844us         0.10%      16.123ms      89.574us           180  \n",
      "                                 aten::zero_         1.02%      10.476ms         1.66%      17.036ms      20.877us       5.585ms         0.62%      16.023ms      19.636us           816  \n",
      "                              aten::_softmax         0.12%       1.262ms         0.26%       2.669ms      47.658us      15.774ms         1.75%      15.878ms     283.544us            56  \n",
      "                                ViewBackward         0.67%       6.894ms         2.74%      28.140ms      48.601us       2.037ms         0.23%      13.906ms      24.017us           579  \n",
      "                                  aten::add_         2.23%      22.901ms         2.23%      22.901ms      25.474us      13.880ms         1.54%      13.880ms      15.439us           899  \n",
      "                                 aten::zeros         0.42%       4.346ms         1.47%      15.113ms      39.357us       4.184ms         0.46%      13.380ms      34.843us           384  \n",
      "                 torch::autograd::CopySlices         0.03%     337.827us         0.29%       3.015ms     167.500us     144.188us         0.02%      12.475ms     693.031us            18  \n",
      "                                  MmBackward         0.20%       2.021ms         1.02%      10.471ms     205.322us     543.406us         0.06%      11.503ms     225.541us            51  \n",
      "                            aten::pin_memory         0.63%       6.449ms         1.15%      11.757ms     335.928us       6.522ms         0.72%      11.110ms     317.436us            35  \n",
      "                                 aten::fill_         0.65%       6.694ms         0.65%       6.694ms       8.056us      10.535ms         1.17%      10.535ms      12.678us           831  \n",
      "                                  aten::sqrt         0.60%       6.174ms         0.99%      10.156ms      18.808us       6.642ms         0.74%       9.630ms      17.834us           540  \n",
      "                                GeluBackward         0.02%     199.256us         0.07%     740.162us      30.840us     103.438us         0.01%       9.536ms     397.344us            24  \n",
      "                         aten::gelu_backward         0.05%     476.917us         0.05%     540.906us      22.538us       9.433ms         1.05%       9.433ms     393.034us            24  \n",
      "                                  aten::mul_         0.79%       8.093ms         0.79%       8.093ms      14.988us       8.561ms         0.95%       8.561ms      15.853us           540  \n",
      "                            aten::zeros_like         0.18%       1.862ms         0.69%       7.057ms      28.006us       3.440ms         0.38%       6.991ms      27.743us           252  \n",
      "                          UnsafeViewBackward         0.64%       6.539ms         1.15%      11.825ms      55.516us     723.250us         0.08%       6.978ms      32.761us           213  \n",
      "                     aten::is_floating_point         0.77%       7.883ms         0.77%       7.883ms       3.343us       6.946ms         0.77%       6.946ms       2.946us          2358  \n",
      "                                  aten::norm         0.55%       5.614ms         0.63%       6.497ms      23.799us       6.669ms         0.74%       6.669ms      24.429us           273  \n",
      "                             PermuteBackward         1.59%      16.332ms         4.07%      41.731ms      51.329us       2.616ms         0.29%       6.582ms       8.096us           813  \n",
      "                                  aten::gelu         0.17%       1.769ms         0.64%       6.554ms      58.518us       2.621ms         0.29%       6.276ms      56.034us           112  \n",
      "                               aten::softmax         0.09%     935.391us         0.67%       6.842ms      61.088us  -12588.688us        -1.40%       6.213ms      55.475us           112  \n",
      "                              aten::addcdiv_         0.35%       3.544ms         0.57%       5.891ms      21.817us       3.867ms         0.43%       6.200ms      22.961us           270  \n",
      "                           AsStridedBackward         0.02%     251.168us         0.15%       1.511ms      83.919us      87.562us         0.01%       6.192ms     344.019us            18  \n",
      "                               IndexBackward         0.02%     220.689us         0.59%       6.099ms     406.612us     198.062us         0.02%       6.116ms     407.760us            15  \n",
      "                              aten::addcmul_         0.34%       3.472ms         0.56%       5.712ms      21.157us       3.816ms         0.42%       6.060ms      22.444us           270  \n",
      "                                 aten::index         0.19%       1.957ms         0.49%       5.006ms      83.436us       2.733ms         0.30%       5.555ms      92.576us            60  \n",
      "                      aten::_index_put_impl_         0.20%       2.045ms         0.52%       5.287ms     352.480us       2.093ms         0.23%       5.095ms     339.660us            15  \n",
      "                                MulBackward0         0.25%       2.523ms         0.42%       4.311ms      95.790us       2.382ms         0.26%       5.033ms     111.851us            45  \n",
      "                                   aten::div         0.46%       4.685ms         0.51%       5.229ms      18.478us       4.969ms         0.55%       4.969ms      17.559us           283  \n",
      "    aten::_amp_non_finite_check_and_unscale_         0.49%       4.992ms         0.49%       4.992ms      18.490us       4.846ms         0.54%       4.846ms      17.948us           270  \n",
      "                                aten::detach         0.36%       3.688ms         0.51%       5.190ms      13.551us       2.598ms         0.29%       3.891ms      10.160us           383  \n",
      "                                   aten::sum         0.15%       1.514ms         0.39%       3.959ms      41.235us       1.536ms         0.17%       3.740ms      38.960us            96  \n",
      "             torch::autograd::AccumulateGrad         0.59%       6.030ms         1.80%      18.480ms      68.446us       1.022ms         0.11%       2.999ms      11.109us           270  \n",
      "                               aten::nonzero         0.18%       1.863ms         0.28%       2.827ms      74.399us       2.096ms         0.23%       2.818ms      74.167us            38  \n",
      "                                 aten::where         0.07%     700.116us         0.28%       2.829ms      34.084us       1.038ms         0.12%       2.673ms      32.207us            83  \n",
      "                     aten::repeat_interleave         0.04%     458.697us         0.26%       2.624ms     187.428us     486.625us         0.05%       2.487ms     177.676us            14  \n",
      "                               aten::addcdiv         0.23%       2.346ms         0.23%       2.346ms       8.690us       2.332ms         0.26%       2.332ms       8.638us           270  \n",
      "                              SWhereBackward         0.03%     340.589us         0.22%       2.300ms     127.794us     341.062us         0.04%       2.294ms     127.458us            18  \n",
      "                               aten::addcmul         0.22%       2.240ms         0.22%       2.240ms       8.298us       2.244ms         0.25%       2.244ms       8.310us           270  \n",
      "                           aten::log_softmax         0.02%     234.809us         0.22%       2.227ms      79.537us     183.750us         0.02%       2.115ms      75.531us            28  \n",
      "                                aten::arange         0.03%     292.858us         0.15%       1.578ms      56.349us       1.161ms         0.13%       1.857ms      66.334us            28  \n",
      "                              aten::_s_where         0.15%       1.569ms         0.17%       1.742ms      20.991us       1.635ms         0.18%       1.635ms      19.704us            83  \n",
      "                             aten::is_pinned         0.15%       1.498ms         0.15%       1.498ms      42.803us       1.376ms         0.15%       1.376ms      39.315us            35  \n",
      "                                 aten::chunk         0.02%     237.307us         0.44%       4.542ms     126.163us     113.562us         0.01%       1.347ms      37.406us            36  \n",
      "                                      detach         0.15%       1.502ms         0.15%       1.502ms       3.920us       1.293ms         0.14%       1.293ms       3.376us           383  \n",
      "                                 aten::split         0.21%       2.198ms         0.42%       4.305ms     119.571us     712.188us         0.08%       1.233ms      34.252us            36  \n",
      "                                    aten::gt         0.02%     241.457us         0.05%     524.555us      37.468us     960.844us         0.11%       1.224ms      87.444us            14  \n",
      "                                AddBackward0         0.18%       1.848ms         0.18%       1.848ms      13.392us       1.175ms         0.13%       1.175ms       8.515us           138  \n",
      "                                    aten::eq         0.07%     695.617us         0.11%       1.127ms      22.531us     714.500us         0.08%       1.064ms      21.271us            50  \n",
      "                                aten::narrow         0.24%       2.420ms         0.37%       3.822ms       6.728us       1.024ms         0.11%       1.024ms       1.802us           568  \n",
      "                                  aten::item         0.04%     419.717us         0.10%     992.953us      14.820us     414.348us         0.05%       1.006ms      15.018us            67  \n",
      "                          UnsqueezeBackward0         0.90%       9.188ms         1.14%      11.668ms      19.065us     995.125us         0.11%     995.125us       1.626us           612  \n",
      "                                aten::unfold         1.02%      10.455ms         1.21%      12.358ms      24.184us     952.312us         0.11%     952.312us       1.864us           511  \n",
      "                          aten::_log_softmax         0.08%     829.503us         0.09%     924.773us      66.055us     877.531us         0.10%     919.406us      65.672us            14  \n",
      "                                 CatBackward         0.13%       1.376ms         0.28%       2.907ms      26.913us     532.750us         0.06%     876.031us       8.111us           108  \n",
      "                                  aten::relu         0.01%     119.059us         0.04%     391.748us      55.964us     163.156us         0.02%     843.250us     120.464us             7  \n",
      "                             aten::remainder         0.06%     634.393us         0.08%     839.762us      25.447us     635.062us         0.07%     754.531us      22.865us            33  \n",
      "                                   aten::neg         0.05%     464.457us         0.07%     755.896us      22.232us     509.094us         0.06%     745.156us      21.916us            34  \n",
      "                              aten::mse_loss         0.03%     277.579us         0.08%     784.317us      56.023us     261.781us         0.03%     723.906us      51.708us            14  \n",
      "                             aten::threshold         0.01%     122.140us         0.03%     272.689us      38.956us     680.094us         0.08%     680.094us      97.156us             7  \n",
      "                   aten::_local_scalar_dense         0.06%     573.236us         0.06%     573.236us       8.556us     591.865us         0.07%     591.865us       8.834us            67  \n",
      "                               ReluBackward0         0.00%      28.129us         0.01%      91.599us      30.533us       9.094us         0.00%     565.219us     188.406us             3  \n",
      "                    aten::threshold_backward         0.01%      56.910us         0.01%      63.470us      21.157us     556.125us         0.06%     556.125us     185.375us             3  \n",
      "                              aten::nll_loss         0.01%     147.689us         0.06%     606.625us      43.330us     -69.469us        -0.01%     553.844us      39.560us            14  \n",
      "                                  aten::rsub         0.02%     159.716us         0.02%     178.726us      25.532us     548.688us         0.06%     548.688us      78.384us             7  \n",
      "                                   aten::sub         0.05%     489.934us         0.05%     549.914us      19.640us     522.438us         0.06%     522.438us      18.658us            28  \n",
      "                         aten::floor_divide_         0.01%     152.188us         0.04%     438.175us      29.212us     151.094us         0.02%     432.781us      28.852us            15  \n",
      "                      aten::nll_loss_forward         0.02%     180.128us         0.02%     180.128us      25.733us     368.656us         0.04%     368.656us      52.665us             7  \n",
      "                          LogSoftmaxBackward         0.01%      70.849us         0.03%     330.477us      55.080us      61.344us         0.01%     310.781us      51.797us             6  \n",
      "                                MaxBackward0         0.00%      31.340us         0.03%     308.437us     102.812us      30.844us         0.00%     307.375us     102.458us             3  \n",
      "                     aten::mse_loss_backward         0.01%     125.928us         0.03%     294.076us      49.013us     124.594us         0.01%     292.906us      48.818us             6  \n",
      "                              aten::randperm         0.01%     137.099us         0.03%     290.138us      72.535us     145.401us         0.02%     290.778us      72.694us             4  \n",
      "                          aten::floor_divide         0.02%     228.047us         0.03%     285.987us      19.066us     227.969us         0.03%     281.688us      18.779us            15  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.025s\n",
      "CUDA time total: 900.187ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", top_level_events_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-australian",
   "metadata": {},
   "source": [
    "# 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "potential-southwest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================\n",
      "This report only display top-level ops statistics\n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                BmmBackward0         0.36%       4.365ms         1.84%      22.509ms     133.984us       1.675ms         0.13%     286.743ms       1.707ms           168  \n",
      "                                aten::einsum         1.59%      19.517ms        11.25%     137.688ms     409.784us      14.264ms         1.10%     156.071ms     464.496us           336  \n",
      "                                  aten::set_         9.06%     110.912ms         9.06%     110.912ms       3.391us     145.859ms        11.22%     145.859ms       4.459us         32709  \n",
      "                                    aten::to         5.13%      62.797ms        15.47%     189.432ms      89.187us      15.396ms         1.18%     116.586ms      54.890us          2124  \n",
      "                                 aten::stack         1.67%      20.384ms         8.61%     105.368ms       2.395ms      71.113ms         5.47%     104.446ms       2.374ms            44  \n",
      "                            aten::layer_norm         0.28%       3.430ms         2.66%      32.598ms     156.719us       3.463ms         0.27%      70.370ms     338.316us           208  \n",
      "                                aten::matmul         0.39%       4.789ms         4.06%      49.718ms     182.787us       5.567ms         0.43%      54.569ms     200.620us           272  \n",
      "                                   aten::add         0.92%      11.238ms         1.03%      12.665ms      21.837us      47.540ms         3.66%      47.540ms      81.965us           580  \n",
      "                                  aten::add_         7.04%      86.220ms         7.04%      86.220ms      71.970us      40.388ms         3.11%      40.388ms      33.713us          1198  \n",
      "                               aten::permute         5.01%      61.368ms         5.25%      64.318ms      19.443us      34.893ms         2.68%      34.893ms      10.548us          3308  \n",
      "              torch::autograd::CopyBackwards         0.45%       5.517ms         4.79%      58.653ms     110.251us       3.306ms         0.25%      32.437ms      60.972us           532  \n",
      "                              RepeatBackward         0.20%       2.419ms         1.34%      16.461ms     342.936us       1.433ms         0.11%      31.045ms     646.772us            48  \n",
      "                                aten::repeat         0.52%       6.333ms         2.71%      33.219ms     319.418us       6.315ms         0.49%      30.695ms     295.139us           104  \n",
      "                     NativeLayerNormBackward         0.22%       2.683ms         0.84%      10.344ms     198.922us     529.531us         0.04%      23.407ms     450.130us            52  \n",
      "                               aten::softmax         0.11%       1.346ms         0.50%       6.158ms      48.108us     997.781us         0.08%      22.459ms     175.463us           128  \n",
      "                               SliceBackward         0.17%       2.030ms        10.47%     128.172ms     534.049us       1.900ms         0.15%      21.186ms      88.277us           240  \n",
      "                                ViewBackward         0.61%       7.503ms         1.82%      22.302ms      28.888us       5.214ms         0.40%      18.802ms      24.356us           772  \n",
      "                                  aten::gelu         0.18%       2.161ms         0.49%       5.980ms      46.721us       6.988ms         0.54%      17.787ms     138.961us           128  \n",
      "                                  MmBackward         0.19%       2.290ms         0.79%       9.676ms     142.289us       1.058ms         0.08%      17.738ms     260.849us            68  \n",
      "                             PermuteBackward         1.01%      12.317ms         2.81%      34.349ms      31.688us       6.960ms         0.54%      16.947ms      15.634us          1084  \n",
      "                                 aten::zeros         0.48%       5.907ms         4.50%      55.108ms     114.808us       3.295ms         0.25%      16.429ms      34.226us           480  \n",
      "                                   aten::mul         0.41%       5.000ms         0.47%       5.714ms      21.977us      14.621ms         1.12%      14.621ms      56.236us           260  \n",
      "                             SoftmaxBackward         0.02%     294.699us         0.17%       2.124ms      66.366us     219.594us         0.02%      14.421ms     450.663us            32  \n",
      "                                  aten::sqrt         0.67%       8.250ms         1.12%      13.759ms      19.110us       9.677ms         0.74%      14.163ms      19.671us           720  \n",
      "                                  aten::mul_         0.92%      11.292ms         0.92%      11.292ms      15.683us      11.040ms         0.85%      11.040ms      15.333us           720  \n",
      "                 torch::autograd::CopySlices         0.04%     506.986us         0.19%       2.305ms      96.052us     409.781us         0.03%       9.673ms     403.055us            24  \n",
      "                               IndexBackward         0.03%     311.188us         0.75%       9.184ms     459.224us     278.469us         0.02%       9.186ms     459.322us            20  \n",
      "                                MulBackward0         0.52%       6.315ms         0.68%       8.287ms     138.117us       6.635ms         0.51%       8.988ms     149.797us            60  \n",
      "                              aten::addcdiv_         0.38%       4.694ms         0.64%       7.881ms      21.890us       5.418ms         0.42%       8.650ms      24.027us           360  \n",
      "             torch::autograd::AccumulateGrad         0.48%       5.835ms         6.64%      81.246ms     225.683us       3.040ms         0.23%       8.543ms      23.731us           360  \n",
      "                     aten::is_floating_point         0.78%       9.565ms         0.78%       9.565ms       3.548us       8.338ms         0.64%       8.338ms       3.093us          2696  \n",
      "                                   aten::div         0.53%       6.431ms         0.59%       7.201ms      19.153us       8.010ms         0.62%       8.010ms      21.303us           376  \n",
      "                                 aten::index         0.20%       2.410ms         0.66%       8.041ms     118.254us       2.692ms         0.21%       7.632ms     112.240us            68  \n",
      "                              aten::addcmul_         0.37%       4.568ms         0.62%       7.546ms      20.961us       4.643ms         0.36%       7.293ms      20.257us           360  \n",
      "                                  aten::norm         0.65%       7.907ms         0.75%       9.181ms      25.221us       6.793ms         0.52%       6.793ms      18.662us           364  \n",
      "                          UnsafeViewBackward         0.43%       5.205ms         0.80%       9.811ms      34.545us       1.747ms         0.13%       6.650ms      23.416us           284  \n",
      "                            aten::zeros_like         0.17%       2.023ms         0.64%       7.811ms      29.147us       2.753ms         0.21%       6.643ms      24.786us           268  \n",
      "                            aten::pin_memory         0.06%     726.442us         0.41%       5.015ms     125.379us       1.429ms         0.11%       6.406ms     160.160us            40  \n",
      "                           AsStridedBackward         0.03%     345.937us         0.15%       1.789ms      74.549us     269.938us         0.02%       5.071ms     211.271us            24  \n",
      "                                   aten::sum         0.15%       1.839ms         0.39%       4.817ms      44.603us       1.868ms         0.14%       4.811ms      44.551us           108  \n",
      "    aten::_amp_non_finite_check_and_unscale_         2.73%      33.409ms         2.73%      33.409ms      92.803us       4.657ms         0.36%       4.657ms      12.935us           360  \n",
      "                                aten::detach         0.68%       8.340ms         0.95%      11.612ms      24.293us       2.933ms         0.23%       4.480ms       9.371us           478  \n",
      "                                 aten::where         0.07%     854.135us         0.38%       4.595ms      45.953us       1.180ms         0.09%       4.089ms      40.886us           100  \n",
      "                                 CatBackward         0.11%       1.391ms         0.28%       3.375ms      23.441us       1.985ms         0.15%       4.056ms      28.166us           144  \n",
      "                                GeluBackward         0.03%     318.638us         0.09%       1.093ms      34.157us     220.656us         0.02%       3.713ms     116.042us            32  \n",
      "                          UnsqueezeBackward0         0.54%       6.655ms         0.80%       9.793ms      12.001us       2.831ms         0.22%       2.831ms       3.469us           816  \n",
      "                              SWhereBackward         0.04%     454.428us         0.26%       3.135ms     130.623us     375.062us         0.03%       2.524ms     105.173us            24  \n",
      "                           aten::log_softmax         0.02%     285.507us         0.13%       1.568ms      49.010us     282.531us         0.02%       2.359ms      73.713us            32  \n",
      "                                   aten::neg         0.05%     566.875us         0.08%     935.002us      23.375us       1.295ms         0.10%       2.267ms      56.670us            40  \n",
      "                     aten::repeat_interleave         0.05%     581.276us         0.28%       3.398ms     212.355us     404.906us         0.03%       2.065ms     129.090us            16  \n",
      "                                    aten::eq         0.07%     863.582us         0.12%       1.416ms      25.283us       1.127ms         0.09%       1.519ms      27.132us            56  \n",
      "                                  aten::item         0.04%     516.659us         0.54%       6.570ms      86.453us     503.492us         0.04%       1.243ms      16.359us            76  \n",
      "                              aten::nll_loss         0.01%     169.399us         0.06%     761.685us      47.605us     158.000us         0.01%       1.112ms      69.504us            16  \n",
      "                               CloneBackward         0.17%       2.126ms         0.17%       2.126ms       9.844us       1.078ms         0.08%       1.078ms       4.992us           216  \n",
      "                                AddBackward0         0.09%       1.105ms         0.09%       1.105ms       6.008us       1.051ms         0.08%       1.051ms       5.713us           184  \n",
      "                                 NegBackward         0.00%      31.630us         0.01%     177.658us      44.414us      28.719us         0.00%     862.250us     215.562us             4  \n",
      "                              aten::mse_loss         0.03%     328.558us         0.07%     909.497us      56.844us     321.250us         0.02%     847.250us      52.953us            16  \n",
      "                                   aten::sub         0.05%     594.436us         0.05%     656.645us      20.520us     631.438us         0.05%     631.438us      19.732us            32  \n",
      "                                    aten::gt         0.04%     515.826us         0.08%     918.671us      57.417us     285.531us         0.02%     607.938us      37.996us            16  \n",
      "                                  aten::relu         0.01%     130.778us         0.02%     284.297us      35.537us      92.344us         0.01%     452.750us      56.594us             8  \n",
      "                          LogSoftmaxBackward         0.01%      83.648us         0.04%     440.225us      55.028us      67.656us         0.01%     368.781us      46.098us             8  \n",
      "                                   TBackward         0.02%     292.747us         0.05%     657.416us       9.668us     360.438us         0.03%     360.438us       5.301us            68  \n",
      "                               ReluBackward0         0.00%      35.330us         0.01%     133.459us      33.365us      25.875us         0.00%     345.312us      86.328us             4  \n",
      "                                aten::arange         0.03%     402.988us         0.38%       4.683ms     146.350us     227.469us         0.02%     335.219us      10.476us            32  \n",
      "                                MaxBackward0         0.00%      39.968us         0.03%     378.467us      94.617us      35.875us         0.00%     315.375us      78.844us             4  \n",
      "                              ExpandBackward         0.01%     158.580us         0.03%     351.709us      10.991us     143.969us         0.01%     303.719us       9.491us            32  \n",
      "                             MseLossBackward         0.00%      44.729us         0.03%     340.197us      85.049us      39.875us         0.00%     293.781us      73.445us             4  \n",
      "                             NllLossBackward         0.00%      48.400us         0.02%     205.690us      51.423us      45.031us         0.00%     288.750us      72.188us             4  \n",
      "                              aten::randperm         0.01%     118.449us         0.02%     257.648us      64.412us     126.789us         0.01%     257.508us      64.377us             4  \n",
      "                               MeanBackward0         0.01%      62.369us         0.02%     289.106us      72.277us      78.812us         0.01%     242.781us      60.695us             4  \n",
      "                                   aten::max         0.02%     240.648us         0.02%     283.878us      35.485us     236.344us         0.02%     236.344us      29.543us             8  \n",
      "                            aten::reciprocal         0.02%     287.049us         0.04%     473.158us      29.572us     169.625us         0.01%     233.375us      14.586us            16  \n",
      "                              aten::linspace         0.01%     146.920us         0.02%     243.929us      15.246us     145.156us         0.01%     215.938us      13.496us            16  \n",
      "                                  aten::rsub         0.02%     198.700us         0.02%     224.350us      28.044us     186.062us         0.01%     186.062us      23.258us             8  \n",
      "                            aten::is_nonzero         0.00%      27.649us         0.45%       5.505ms       1.376ms      27.125us         0.00%     177.000us      44.250us             4  \n",
      "                                DivBackward0         0.00%      55.040us         0.01%     168.098us      42.025us      48.188us         0.00%     142.281us      35.570us             4  \n",
      "                                    aten::lt         0.01%     129.658us         0.02%     198.978us      24.872us      88.062us         0.01%     120.812us      15.102us             8  \n",
      "                                aten::argmax         0.01%      92.990us         0.01%     109.320us      27.330us     110.250us         0.01%     110.250us      27.562us             4  \n",
      "                     aten::_amp_update_scale         0.01%      76.889us         0.01%     106.378us      26.594us     108.750us         0.01%     108.750us      27.188us             4  \n",
      "                             aten::ones_like         0.00%      33.500us         0.01%      89.029us      22.257us      52.219us         0.00%      89.062us      22.266us             4  \n",
      "                                  aten::full         0.00%      49.480us         0.01%     166.039us      41.510us      29.312us         0.00%      79.625us      19.906us             4  \n",
      "                               aten::random_         0.01%      94.850us         0.01%      94.850us      23.713us      78.133us         0.01%      78.133us      19.533us             4  \n",
      "                                SumBackward1         0.00%      27.600us         0.01%      78.720us      19.680us      63.625us         0.00%      63.625us      15.906us             4  \n",
      "                                SubBackward0         0.00%      46.120us         0.00%      46.120us       3.843us      44.125us         0.00%      44.125us       3.677us            12  \n",
      "                            SqueezeBackward1         0.00%      22.660us         0.00%      51.910us      12.977us      36.875us         0.00%      36.875us       9.219us             4  \n",
      "                          TransposeBackward0         0.00%      23.640us         0.00%      53.450us      13.362us      36.844us         0.00%      36.844us       9.211us             4  \n",
      "                     aten::broadcast_tensors         0.00%      32.430us         0.00%      32.430us       4.054us      25.188us         0.00%      25.188us       3.148us             8  \n",
      "                  torch::autograd::GraphRoot         0.00%      18.319us         0.00%      18.319us       4.580us      15.344us         0.00%      15.344us       3.836us             4  \n",
      "                                 aten::empty         3.59%      43.979ms         3.59%      43.979ms       1.147us       0.000us         0.00%       0.000us       0.000us         38331  \n",
      "                                     aten::t         0.12%       1.437ms         0.20%       2.414ms       5.389us       0.000us         0.00%       0.000us       0.000us           448  \n",
      "                               aten::squeeze         0.21%       2.585ms         0.26%       3.195ms       3.878us       0.000us         0.00%       0.000us       0.000us           824  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.224s\n",
      "CUDA time total: 1.300s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", top_level_events_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sonic-confidentiality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                   aten::bmm         3.00%      36.771ms         6.67%      81.645ms      80.997us     330.276ms        25.41%     401.581ms     398.394us          1008  \n",
      "                                BmmBackward0         0.36%       4.365ms         1.84%      22.509ms     133.984us       1.675ms         0.13%     286.743ms       1.707ms           168  \n",
      "                                 aten::copy_        17.88%     218.823ms        17.88%     218.840ms      75.151us     208.291ms        16.02%     208.291ms      71.529us          2912  \n",
      "                                aten::einsum         1.59%      19.517ms        11.25%     137.688ms     409.784us      14.264ms         1.10%     156.071ms     464.496us           336  \n",
      "                                  aten::set_         9.06%     110.912ms         9.06%     110.912ms       3.391us     145.859ms        11.22%     145.859ms       4.459us         32709  \n",
      "                                    aten::to         5.13%      62.797ms        15.47%     189.432ms      89.187us      15.396ms         1.18%     116.586ms      54.890us          2124  \n",
      "                                 aten::stack         1.67%      20.384ms         8.61%     105.368ms       2.395ms      71.113ms         5.47%     104.446ms       2.374ms            44  \n",
      "                                   aten::cat         1.01%      12.305ms         5.91%      72.300ms     116.613us      12.848ms         0.99%      75.661ms     122.034us           620  \n",
      "                               aten::reshape         1.48%      18.060ms         4.25%      52.078ms      21.627us      12.188ms         0.94%      71.451ms      29.672us          2408  \n",
      "                            aten::layer_norm         0.28%       3.430ms         2.66%      32.598ms     156.719us       3.463ms         0.27%      70.370ms     338.316us           208  \n",
      "                                 aten::clone         1.01%      12.339ms         2.15%      26.331ms      40.634us      11.729ms         0.90%      62.846ms      96.985us           648  \n",
      "                                aten::matmul         0.39%       4.789ms         4.06%      49.718ms     182.787us       5.567ms         0.43%      54.569ms     200.620us           272  \n",
      "                                   aten::add         0.92%      11.238ms         1.03%      12.665ms      21.837us      47.540ms         3.66%      47.540ms      81.965us           580  \n",
      "                                  aten::_cat         2.99%      36.634ms         3.40%      41.611ms     125.334us      43.254ms         3.33%      43.645ms     131.460us           332  \n",
      "                                  aten::add_         7.04%      86.220ms         7.04%      86.220ms      71.970us      40.388ms         3.11%      40.388ms      33.713us          1198  \n",
      "                               aten::permute         5.01%      61.368ms         5.25%      64.318ms      19.443us      34.893ms         2.68%      34.893ms      10.548us          3308  \n",
      "              torch::autograd::CopyBackwards         0.45%       5.517ms         4.79%      58.653ms     110.251us       3.306ms         0.25%      32.437ms      60.972us           532  \n",
      "                              RepeatBackward         0.20%       2.419ms         1.34%      16.461ms     342.936us       1.433ms         0.11%      31.045ms     646.772us            48  \n",
      "                                aten::repeat         0.52%       6.333ms         2.71%      33.219ms     319.418us       6.315ms         0.49%      30.695ms     295.139us           104  \n",
      "                                    aten::mm         1.14%      13.984ms         1.21%      14.848ms      55.402us      25.710ms         1.98%      25.710ms      95.933us           268  \n",
      "                            aten::contiguous         0.25%       3.053ms         0.97%      11.894ms      22.357us       2.744ms         0.21%      24.663ms      46.360us           532  \n",
      "                                 aten::zero_         4.03%      49.283ms         4.72%      57.788ms      56.766us      11.953ms         0.92%      23.455ms      23.040us          1018  \n",
      "                     NativeLayerNormBackward         0.22%       2.683ms         0.84%      10.344ms     198.922us     529.531us         0.04%      23.407ms     450.130us            52  \n",
      "                               aten::softmax         0.11%       1.346ms         0.50%       6.158ms      48.108us     997.781us         0.08%      22.459ms     175.463us           128  \n",
      "                               SliceBackward         0.17%       2.030ms        10.47%     128.172ms     534.049us       1.900ms         0.15%      21.186ms      88.277us           240  \n",
      "                        aten::slice_backward         0.33%       4.018ms        10.30%     126.142ms     525.590us       2.350ms         0.18%      19.287ms      80.362us           240  \n",
      "                                ViewBackward         0.61%       7.503ms         1.82%      22.302ms      28.888us       5.214ms         0.40%      18.802ms      24.356us           772  \n",
      "                                  aten::gelu         0.18%       2.161ms         0.49%       5.980ms      46.721us       6.988ms         0.54%      17.787ms     138.961us           128  \n",
      "                                  MmBackward         0.19%       2.290ms         0.79%       9.676ms     142.289us       1.058ms         0.08%      17.738ms     260.849us            68  \n",
      "            aten::native_layer_norm_backward         0.28%       3.374ms         0.33%       4.015ms      77.204us      17.099ms         1.32%      17.099ms     328.833us            52  \n",
      "                             PermuteBackward         1.01%      12.317ms         2.81%      34.349ms      31.688us       6.960ms         0.54%      16.947ms      15.634us          1084  \n",
      "                                 aten::zeros         0.48%       5.907ms         4.50%      55.108ms     114.808us       3.295ms         0.25%      16.429ms      34.226us           480  \n",
      "                     aten::native_layer_norm         0.55%       6.718ms         0.60%       7.335ms      70.527us      15.691ms         1.21%      15.691ms     150.878us           104  \n",
      "                                   aten::mul         0.41%       5.000ms         0.47%       5.714ms      21.977us      14.621ms         1.12%      14.621ms      56.236us           260  \n",
      "                             SoftmaxBackward         0.02%     294.699us         0.17%       2.124ms      66.366us     219.594us         0.02%      14.421ms     450.663us            32  \n",
      "                aten::_softmax_backward_data         0.08%     987.050us         0.15%       1.829ms      57.157us       6.828ms         0.53%      14.202ms     443.801us            32  \n",
      "                                  aten::sqrt         0.67%       8.250ms         1.12%      13.759ms      19.110us       9.677ms         0.74%      14.163ms      19.671us           720  \n",
      "                                 aten::fill_         0.71%       8.665ms         0.71%       8.665ms       8.380us      11.612ms         0.89%      11.612ms      11.230us          1034  \n",
      "                                  aten::mul_         0.92%      11.292ms         0.92%      11.292ms      15.683us      11.040ms         0.85%      11.040ms      15.333us           720  \n",
      "                              aten::_softmax         0.13%       1.537ms         0.17%       2.067ms      32.299us      10.238ms         0.79%      10.431ms     162.980us            64  \n",
      "                 torch::autograd::CopySlices         0.04%     506.986us         0.19%       2.305ms      96.052us     409.781us         0.03%       9.673ms     403.055us            24  \n",
      "                                aten::unfold         0.52%       6.334ms         1.94%      23.796ms      40.746us       9.658ms         0.74%       9.658ms      16.538us           584  \n",
      "                               IndexBackward         0.03%     311.188us         0.75%       9.184ms     459.224us     278.469us         0.02%       9.186ms     459.322us            20  \n",
      "                                MulBackward0         0.52%       6.315ms         0.68%       8.287ms     138.117us       6.635ms         0.51%       8.988ms     149.797us            60  \n",
      "                              aten::addcdiv_         0.38%       4.694ms         0.64%       7.881ms      21.890us       5.418ms         0.42%       8.650ms      24.027us           360  \n",
      "             torch::autograd::AccumulateGrad         0.48%       5.835ms         6.64%      81.246ms     225.683us       3.040ms         0.23%       8.543ms      23.731us           360  \n",
      "                     aten::is_floating_point         0.78%       9.565ms         0.78%       9.565ms       3.548us       8.338ms         0.64%       8.338ms       3.093us          2696  \n",
      "                      aten::_index_put_impl_         0.29%       3.554ms         0.66%       8.058ms     402.920us       3.973ms         0.31%       8.017ms     400.831us            20  \n",
      "                                   aten::div         0.53%       6.431ms         0.59%       7.201ms      19.153us       8.010ms         0.62%       8.010ms      21.303us           376  \n",
      "                                 aten::index         0.20%       2.410ms         0.66%       8.041ms     118.254us       2.692ms         0.21%       7.632ms     112.240us            68  \n",
      "                              aten::addcmul_         0.37%       4.568ms         0.62%       7.546ms      20.961us       4.643ms         0.36%       7.293ms      20.257us           360  \n",
      "                                  aten::norm         0.65%       7.907ms         0.75%       9.181ms      25.221us       6.793ms         0.52%       6.793ms      18.662us           364  \n",
      "                          UnsafeViewBackward         0.43%       5.205ms         0.80%       9.811ms      34.545us       1.747ms         0.13%       6.650ms      23.416us           284  \n",
      "                            aten::zeros_like         0.17%       2.023ms         0.64%       7.811ms      29.147us       2.753ms         0.21%       6.643ms      24.786us           268  \n",
      "                            aten::pin_memory         0.06%     726.442us         0.41%       5.015ms     125.379us       1.429ms         0.11%       6.406ms     160.160us            40  \n",
      "                               aten::nonzero         0.35%       4.276ms         0.45%       5.506ms     125.127us       3.853ms         0.30%       5.129ms     116.570us            44  \n",
      "                           AsStridedBackward         0.03%     345.937us         0.15%       1.789ms      74.549us     269.938us         0.02%       5.071ms     211.271us            24  \n",
      "                                   aten::sum         0.15%       1.839ms         0.39%       4.817ms      44.603us       1.868ms         0.14%       4.811ms      44.551us           108  \n",
      "    aten::_amp_non_finite_check_and_unscale_         2.73%      33.409ms         2.73%      33.409ms      92.803us       4.657ms         0.36%       4.657ms      12.935us           360  \n",
      "                                aten::detach         0.68%       8.340ms         0.95%      11.612ms      24.293us       2.933ms         0.23%       4.480ms       9.371us           478  \n",
      "                                 aten::chunk         0.03%     334.638us         0.47%       5.714ms     119.044us     291.094us         0.02%       4.160ms      86.658us            48  \n",
      "                                 aten::where         0.07%     854.135us         0.38%       4.595ms      45.953us       1.180ms         0.09%       4.089ms      40.886us           100  \n",
      "                                 CatBackward         0.11%       1.391ms         0.28%       3.375ms      23.441us       1.985ms         0.15%       4.056ms      28.166us           144  \n",
      "                                 aten::split         0.24%       2.960ms         0.44%       5.379ms     112.072us       2.469ms         0.19%       3.869ms      80.594us            48  \n",
      "                                aten::narrow         0.23%       2.872ms         0.38%       4.626ms       6.151us       3.860ms         0.30%       3.860ms       5.134us           752  \n",
      "                                GeluBackward         0.03%     318.638us         0.09%       1.093ms      34.157us     220.656us         0.02%       3.713ms     116.042us            32  \n",
      "                         aten::gelu_backward         0.06%     686.775us         0.06%     774.401us      24.200us       3.493ms         0.27%       3.493ms     109.146us            32  \n",
      "                               aten::addcdiv         0.26%       3.186ms         0.26%       3.186ms       8.851us       3.232ms         0.25%       3.232ms       8.977us           360  \n",
      "                              aten::_s_where         0.25%       3.031ms         0.27%       3.262ms      32.616us       2.909ms         0.22%       2.909ms      29.088us           100  \n",
      "                          UnsqueezeBackward0         0.54%       6.655ms         0.80%       9.793ms      12.001us       2.831ms         0.22%       2.831ms       3.469us           816  \n",
      "                               aten::addcmul         0.24%       2.978ms         0.24%       2.978ms       8.273us       2.649ms         0.20%       2.649ms       7.359us           360  \n",
      "                              SWhereBackward         0.04%     454.428us         0.26%       3.135ms     130.623us     375.062us         0.03%       2.524ms     105.173us            24  \n",
      "                           aten::log_softmax         0.02%     285.507us         0.13%       1.568ms      49.010us     282.531us         0.02%       2.359ms      73.713us            32  \n",
      "                                   aten::neg         0.05%     566.875us         0.08%     935.002us      23.375us       1.295ms         0.10%       2.267ms      56.670us            40  \n",
      "                     aten::repeat_interleave         0.05%     581.276us         0.28%       3.398ms     212.355us     404.906us         0.03%       2.065ms     129.090us            16  \n",
      "                                      detach         0.27%       3.272ms         0.27%       3.272ms       6.845us       1.547ms         0.12%       1.547ms       3.236us           478  \n",
      "                                    aten::eq         0.07%     863.582us         0.12%       1.416ms      25.283us       1.127ms         0.09%       1.519ms      27.132us            56  \n",
      "                                  aten::item         0.04%     516.659us         0.54%       6.570ms      86.453us     503.492us         0.04%       1.243ms      16.359us            76  \n",
      "                              aten::nll_loss         0.01%     169.399us         0.06%     761.685us      47.605us     158.000us         0.01%       1.112ms      69.504us            16  \n",
      "                               CloneBackward         0.17%       2.126ms         0.17%       2.126ms       9.844us       1.078ms         0.08%       1.078ms       4.992us           216  \n",
      "                                AddBackward0         0.09%       1.105ms         0.09%       1.105ms       6.008us       1.051ms         0.08%       1.051ms       5.713us           184  \n",
      "                             aten::remainder         0.07%     864.674us         0.09%       1.147ms      26.071us     860.250us         0.07%       1.037ms      23.577us            44  \n",
      "                          aten::_log_softmax         0.03%     420.018us         0.05%     554.267us      34.642us     895.812us         0.07%     951.281us      59.455us            16  \n",
      "                                 NegBackward         0.00%      31.630us         0.01%     177.658us      44.414us      28.719us         0.00%     862.250us     215.562us             4  \n",
      "                              aten::mse_loss         0.03%     328.558us         0.07%     909.497us      56.844us     321.250us         0.02%     847.250us      52.953us            16  \n",
      "                   aten::_local_scalar_dense         0.49%       6.054ms         0.49%       6.054ms      79.655us     739.812us         0.06%     739.812us       9.734us            76  \n",
      "                                   aten::sub         0.05%     594.436us         0.05%     656.645us      20.520us     631.438us         0.05%     631.438us      19.732us            32  \n",
      "                                    aten::gt         0.04%     515.826us         0.08%     918.671us      57.417us     285.531us         0.02%     607.938us      37.996us            16  \n",
      "                         aten::floor_divide_         0.02%     197.079us         0.05%     577.918us      28.896us     172.969us         0.01%     546.844us      27.342us            20  \n",
      "                                  aten::relu         0.01%     130.778us         0.02%     284.297us      35.537us      92.344us         0.01%     452.750us      56.594us             8  \n",
      "                      aten::nll_loss_forward         0.02%     238.989us         0.02%     238.989us      29.874us     423.781us         0.03%     423.781us      52.973us             8  \n",
      "                          aten::floor_divide         0.02%     303.139us         0.03%     380.839us      19.042us     303.156us         0.02%     373.875us      18.694us            20  \n",
      "                             aten::is_pinned         0.02%     283.720us         0.02%     283.720us       7.093us     370.312us         0.03%     370.312us       9.258us            40  \n",
      "                          LogSoftmaxBackward         0.01%      83.648us         0.04%     440.225us      55.028us      67.656us         0.01%     368.781us      46.098us             8  \n",
      "                                   TBackward         0.02%     292.747us         0.05%     657.416us       9.668us     360.438us         0.03%     360.438us       5.301us            68  \n",
      "                             aten::threshold         0.01%     126.670us         0.01%     153.519us      19.190us     360.406us         0.03%     360.406us      45.051us             8  \n",
      "                             aten::expand_as         0.03%     387.097us         0.05%     624.206us       6.002us     352.031us         0.03%     352.031us       3.385us           104  \n",
      "                               ReluBackward0         0.00%      35.330us         0.01%     133.459us      33.365us      25.875us         0.00%     345.312us      86.328us             4  \n",
      "                                aten::arange         0.03%     402.988us         0.38%       4.683ms     146.350us     227.469us         0.02%     335.219us      10.476us            32  \n",
      "                                 aten::alias         0.03%     388.968us         0.03%     388.968us       3.740us     334.344us         0.03%     334.344us       3.215us           104  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.224s\n",
      "CUDA time total: 1.300s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", top_level_events_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finished-pitch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[95.8308, 66.5633,  3.2623, 42.4742, 67.8822],\n",
       "          [ 8.7043, 77.1832, 79.8523, 86.9034, 44.9597],\n",
       "          [15.1022, 72.9430, 39.1444, 33.3362, 25.7939]],\n",
       "\n",
       "         [[30.1182, 45.9560, 92.7316, 88.5362, 64.0273],\n",
       "          [ 2.8302, 62.9300, 74.0908, 49.1052, 75.3173],\n",
       "          [32.3284, 43.1864, 18.7067,  6.1603, 45.6905]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3, 5)\n",
    "model = MHSA(shape[0], 1, shape[1], shape[2])\n",
    "data = torch.rand(shape).unsqueeze(0) * 100.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cooperative-block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6172,  0.6434,  0.5773,  0.6990,  0.5834],\n",
      "          [ 0.6151,  0.6379,  0.4617,  0.6861,  0.5925],\n",
      "          [ 0.5906,  0.5938,  0.5608,  0.6210,  0.5616]],\n",
      "\n",
      "         [[-1.4552, -1.4575, -1.3866, -1.6590, -1.2823],\n",
      "          [-1.4280, -1.4399, -1.1102, -1.6299, -1.3441],\n",
      "          [-1.3922, -1.3526, -1.3096, -1.4443, -1.2828]]]],\n",
      "       grad_fn=<PermuteBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5906,  0.5938,  0.5608,  0.6210,  0.5616],\n",
       "          [ 0.6172,  0.6434,  0.5773,  0.6990,  0.5834],\n",
       "          [ 0.6151,  0.6379,  0.4617,  0.6861,  0.5925]],\n",
       "\n",
       "         [[-1.3922, -1.3526, -1.3096, -1.4443, -1.2828],\n",
       "          [-1.4552, -1.4575, -1.3866, -1.6590, -1.2823],\n",
       "          [-1.4280, -1.4399, -1.1102, -1.6299, -1.3441]]]],\n",
       "       grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model(data))\n",
    "model(torch.roll(data, 1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incoming-communist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 4.4709, 24.3366, 68.9681, 47.6182, 26.2220],\n",
      "          [ 3.7974, 57.9255, 36.1684, 50.0011,  7.2832],\n",
      "          [18.5523, 19.8874, 10.1587, 85.8722, 18.3180]],\n",
      "\n",
      "         [[76.2447, 80.4759, 76.0635, 76.1226, 19.7503],\n",
      "          [98.2471, 50.4327, 98.9033, 31.4406, 75.8802],\n",
      "          [80.7123, 37.0935, 81.8542,  8.0566,  4.0004]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[18.5523, 19.8874, 10.1587, 85.8722, 18.3180],\n",
       "          [ 4.4709, 24.3366, 68.9681, 47.6182, 26.2220],\n",
       "          [ 3.7974, 57.9255, 36.1684, 50.0011,  7.2832]],\n",
       "\n",
       "         [[80.7123, 37.0935, 81.8542,  8.0566,  4.0004],\n",
       "          [76.2447, 80.4759, 76.0635, 76.1226, 19.7503],\n",
       "          [98.2471, 50.4327, 98.9033, 31.4406, 75.8802]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data)\n",
    "torch.roll(data, 1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smoking-protection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([test, test[:-1]], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "complimentary-direction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0629, 0.1710, 0.7662]),\n",
       " tensor([0.0629, 0.1710, 0.7662]),\n",
       " tensor([0.0064, 0.0471, 0.9465]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([-1, 1., 4.])\n",
    "temp = 0.5\n",
    "F.softmax(temp * F.log_softmax(t, dim=-1), dim=-1), F.softmax(temp * t, dim=-1), F.softmax(t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-immigration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collaborative-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "previous-mongolia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f99f8c45d10>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_batches = int(1e5)\n",
    "starting_lr = 0.0025\n",
    "min_lr = 0.0001\n",
    "gamma = math.exp(math.log(min_lr / starting_lr) / n_batches)\n",
    "\n",
    "result = [starting_lr]\n",
    "for i in range(n_batches):\n",
    "    result.append(result[-1] * gamma)\n",
    "    \n",
    "plt.plot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demonstrated-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Action, Configuration, Observation, histogram, translate, row_col\n",
    "from kaggle_environments import make as kaggle_make\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from hungry_geese.nns import models, conv_blocks\n",
    "from hungry_geese.env.torch_env import TorchEnv\n",
    "from hungry_geese.mcts.torch_mcts import TorchMCTS\n",
    "import hungry_geese.env.goose_env as ge\n",
    "from hungry_geese.mcts.utils import torch_actor_critic_factory\n",
    "from hungry_geese.utils import torch_terminal_value_func\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "config = Configuration(kaggle_make('hungry_geese', debug=False).configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "radical-march",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/isaiah/goose_agent/cp.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-26200c2a3654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullConvActorCriticNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'goose_agent/cp.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt17/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt17/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pt17/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/isaiah/goose_agent/cp.pt'"
     ]
    }
   ],
   "source": [
    "obs_type = ge.ObsType.COMBINED_GRADIENT_OBS_LARGE\n",
    "n_channels = 92\n",
    "activation = nn.ReLU\n",
    "normalize = False\n",
    "use_mhsa = False\n",
    "model_kwargs = dict(\n",
    "    block_class=conv_blocks.BasicConvolutionalBlock,\n",
    "    block_kwargs=[\n",
    "        dict(\n",
    "            in_channels=obs_type.get_obs_spec()[-3],\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize,\n",
    "            use_mhsa=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize,\n",
    "            use_mhsa=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize,\n",
    "            use_mhsa=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize,\n",
    "            use_mhsa=use_mhsa,\n",
    "            mhsa_heads=4,\n",
    "        ),\n",
    "    ],\n",
    "    squeeze_excitation=True,\n",
    "    cross_normalize_value=True,\n",
    "    use_separate_action_value_heads=True,\n",
    "    # **ge.RewardType.RANK_ON_DEATH.get_recommended_value_activation_scale_shift_dict()\n",
    ")\n",
    "model = models.FullConvActorCriticNetwork(**model_kwargs)\n",
    "model.load_state_dict(torch.load(Path.home() / 'goose_agent/cp.pt'))\n",
    "model.to(device)\n",
    "\n",
    "env_kwargs = dict(\n",
    "    config=config,\n",
    "    n_envs=2,\n",
    "    obs_type=obs_type,\n",
    "    device=device\n",
    ")\n",
    "env = TorchEnv(**env_kwargs)\n",
    "env_copy = TorchEnv(**env_kwargs)\n",
    "mcts = TorchMCTS(\n",
    "    torch_actor_critic_factory(model),\n",
    "    torch_terminal_value_func,\n",
    "    100,\n",
    "    add_noise=True,\n",
    "    device=device,\n",
    "    n_envs=env.n_envs,\n",
    "    max_size=250\n",
    ")\n",
    "tree = mcts.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suitable-detector",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:00<00:00, 35.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 29.24it/s]\n",
      " 27%|██▋       | 3/11 [00:00<00:00, 26.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 19.64it/s]\n",
      " 18%|█▊        | 2/11 [00:00<00:00, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 17.78it/s]\n",
      " 18%|█▊        | 2/11 [00:00<00:00, 17.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 22.26it/s]\n",
      " 36%|███▋      | 4/11 [00:00<00:00, 37.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 33.37it/s]\n"
     ]
    }
   ],
   "source": [
    "n_mcts_iter = 100\n",
    "while not env.dones.any():\n",
    "    print(env.step_counters[0].item())\n",
    "    mcts.reset()\n",
    "    mcts.run_mcts(env, env_copy, n_mcts_iter, show_progress=True)\n",
    "    policies = mcts.tree.get_improved_policies()\n",
    "    policy_max = policies.max(dim=-1, keepdim=True).values\n",
    "    actions = torch.multinomial(\n",
    "        (policies == policy_max).view(-1, 4).to(torch.float32),\n",
    "        1\n",
    "    ).view(-1, env.n_geese)\n",
    "    env.step(torch.where(\n",
    "        env.alive,\n",
    "        actions,\n",
    "        torch.zeros_like(actions)\n",
    "    ))\n",
    "    obs_dict = env.generate_obs_dicts()\n",
    "    for env_idx in range(env.n_envs):\n",
    "        assert np.allclose(\n",
    "            env.obs[env_idx].cpu().numpy(),\n",
    "            ge.create_obs_tensor(obs_dict[env_idx], env.obs_type)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surprising-mouth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:01<00:00, 51.18it/s]\n"
     ]
    }
   ],
   "source": [
    "mcts.reset()\n",
    "env.reset()\n",
    "profiling_enabled = False\n",
    "n_mcts_iter = 100\n",
    "with torch.autograd.profiler.profile(enabled=profiling_enabled, use_cuda=True) as prof:\n",
    "    mcts.run_mcts(env, env_copy, n_mcts_iter, show_progress=True)\n",
    "#if profiling_enabled:\n",
    "#    prof.export_chrome_trace(f'trace_{str(device).split(\":\")[0]}_{env.n_envs}_envs_{n_mcts_iter}_iter.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "german-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Lengths: [1, 1, 1, 1]\n",
      "Rewards: [0, 0, 0, 0]\n",
      "\n",
      "_ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ * _ _ C _\n",
      "_ B _ _ _ D _ _ _ _ _\n",
      "_ _ _ _ _ _ _ A _ _ _\n",
      "_ _ _ _ _ _ _ _ _ * _\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1200, 0.7600, 0.0100, 0.1100],\n",
       "        [0.1200, 0.2800, 0.0100, 0.5900],\n",
       "        [0.1700, 0.3800, 0.0100, 0.4400],\n",
       "        [0.3300, 0.5200, 0.0100, 0.1400]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_idx = 0\n",
    "print(env.render_env(env_idx, include_info=True))\n",
    "mcts.tree.get_improved_policies(1.)[env_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "square-darwin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'index': 0,\n",
       "   'step': 0,\n",
       "   'geese': [[52], [67], [31], [55]],\n",
       "   'food': [51, 68]},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'index': 1},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'index': 2},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'index': 3},\n",
       "  'status': 'ACTIVE'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_dict = env.generate_obs_dicts()\n",
    "obs_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vertical-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825 ms ± 1.82 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit env.generate_obs_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alternative-inspector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.2 s ± 526 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'obs_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5ae5203f05dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'obs_dict = env.generate_obs_dicts()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobs_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'obs_dict' is not defined"
     ]
    }
   ],
   "source": [
    "%timeit obs_dict = env.generate_obs_dicts()\n",
    "obs_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "closed-tissue",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "           mcts_backpropagate         0.16%      21.880ms        10.99%        1.473s      14.586ms       8.015ms         0.01%       51.872s     513.587ms           101  \n",
      "                   aten::item         0.60%      79.913ms         6.04%     810.280ms      85.880us       50.862s        79.20%       51.649s       5.474ms          9435  \n",
      "             aten::is_nonzero         0.08%      10.221ms         1.49%     200.264ms      92.801us      13.207ms         0.02%       50.964s      23.616ms          2158  \n",
      "                  mcts_expand         2.38%     318.850ms        75.57%       10.134s     100.333ms     253.241ms         0.39%        9.870s      97.721ms           101  \n",
      "                     env_step         9.69%        1.299s        63.45%        8.509s      26.101ms     239.937ms         0.37%        8.109s      24.873ms           326  \n",
      "                aten::nonzero        21.27%        2.852s        25.16%        3.375s     128.401us        3.680s         5.73%        3.866s     147.081us         26282  \n",
      "                  aten::index         7.60%        1.019s        32.40%        4.345s     125.539us     529.163ms         0.82%        3.744s     108.169us         34611  \n",
      "                    mcts_eval         3.22%     431.776ms        13.11%        1.757s      17.400ms     341.356ms         0.53%        2.457s      24.330ms           101  \n",
      "      aten::repeat_interleave         1.28%     171.374ms        12.59%        1.689s     410.457us  -235950.765us        -0.37%        1.234s     299.877us          4114  \n",
      "             aten::index_put_         1.05%     140.735ms        11.23%        1.506s     110.966us      94.064ms         0.15%        1.164s      85.773us         13568  \n",
      "       aten::_index_put_impl_         2.44%     327.676ms        10.18%        1.365s     100.594us     169.585ms         0.26%        1.070s      78.840us         13568  \n",
      "                     aten::to         2.05%     275.369ms         4.29%     575.249ms       6.150us     383.558ms         0.60%        1.031s      11.024us         93538  \n",
      "                update_values         1.12%     150.414ms         7.60%        1.019s       3.125ms   -7144.570us        -0.01%     844.162ms       2.589ms           326  \n",
      "                  aten::copy_         3.17%     425.483ms         3.17%     425.521ms      16.795us     803.937ms         1.25%     803.937ms      31.731us         25336  \n",
      "    aten::_local_scalar_dense         5.45%     730.366ms         5.45%     730.366ms      77.410us     787.355ms         1.23%     787.355ms      83.450us          9435  \n",
      "                    aten::add         1.16%     155.587ms         1.28%     171.267ms      32.778us     511.731ms         0.80%     511.731ms      97.939us          5225  \n",
      "                    aten::sum         1.11%     148.969ms         1.62%     216.801ms      42.965us     417.327ms         0.65%     439.803ms      87.159us          5046  \n",
      "                  aten::where         0.56%      75.342ms         1.34%     180.284ms      48.476us     282.369ms         0.44%     321.700ms      86.502us          3719  \n",
      "                     aten::eq         0.80%     106.721ms         1.29%     172.908ms      29.936us     293.095ms         0.46%     314.364ms      54.426us          5776  \n",
      "            aten::multinomial         0.46%      62.009ms         2.85%     382.681ms     586.934us   -3945.781us        -0.01%     280.074ms     429.561us           652  \n",
      "                    aten::mul         0.69%      92.556ms         0.76%     101.961ms      34.645us     258.510ms         0.40%     258.510ms      87.839us          2943  \n",
      "              aten::remainder         0.65%      87.829ms         0.77%     103.908ms      45.534us     253.951ms         0.40%     258.409ms     113.238us          2282  \n",
      "                aten::__and__         0.54%      72.799ms         1.29%     172.646ms      48.145us     204.873ms         0.32%     247.304ms      68.964us          3586  \n",
      "                   aten::rsub         0.36%      48.399ms         0.40%      53.001ms      37.723us     240.470ms         0.37%     240.470ms     171.153us          1405  \n",
      "                    aten::all         0.65%      86.815ms         0.81%     108.323ms      27.854us     237.291ms         0.37%     238.438ms      61.311us          3889  \n",
      "                     aten::gt         0.60%      81.064ms         0.89%     118.829ms      30.737us     215.722ms         0.34%     225.619ms      58.360us          3866  \n",
      "                 aten::arange         0.30%      40.819ms         0.46%      61.995ms      24.640us     205.980ms         0.32%     217.937ms      86.620us          2516  \n",
      "                    aten::div         0.44%      58.679ms         0.48%      64.544ms      33.391us     206.913ms         0.32%     206.913ms     107.043us          1933  \n",
      "                go_to_parents         0.42%      56.035ms         2.61%     350.517ms       1.075ms  -170846.172us        -0.27%     200.816ms     615.999us           326  \n",
      "             aten::contiguous         1.39%     186.067ms         2.42%     324.922ms       6.110us     115.154ms         0.18%     192.782ms       3.625us         53177  \n",
      "                   aten::add_         0.51%      68.979ms         0.51%      68.979ms      23.438us     184.513ms         0.29%     184.513ms      62.696us          2943  \n",
      "                    aten::sub         0.33%      44.873ms         0.37%      48.988ms      34.091us     182.413ms         0.28%     182.413ms     126.940us          1437  \n",
      "                 aten::conv2d         0.20%      27.034ms         1.45%     194.898ms     175.425us      24.382ms         0.04%     179.797ms     161.834us          1111  \n",
      "                  aten::relu_         0.28%      37.515ms         0.39%      51.942ms      34.285us     153.301ms         0.24%     162.542ms     107.289us          1515  \n",
      "                aten::reshape         2.12%     284.101ms         3.64%     487.919ms       5.266us     161.072ms         0.25%     161.072ms       1.738us         92657  \n",
      "             aten::zeros_like         0.20%      26.288ms         0.37%      49.981ms      43.199us     151.943ms         0.24%     160.953ms     139.113us          1157  \n",
      "            aten::convolution         0.05%       6.557ms         1.25%     167.864ms     151.093us       3.766ms         0.01%     155.415ms     139.888us          1111  \n",
      "           aten::_convolution         0.16%      21.115ms         1.20%     161.307ms     145.190us       9.525ms         0.01%     151.649ms     136.498us          1111  \n",
      "                     aten::lt         0.39%      52.160ms         0.61%      81.847ms      25.106us     133.641ms         0.21%     141.187ms      43.309us          3260  \n",
      "                  aten::zeros         0.24%      32.637ms         0.47%      63.638ms      27.231us     118.275ms         0.18%     138.369ms      59.208us          2337  \n",
      "                   aten::set_         1.70%     227.778ms         1.70%     227.778ms       8.667us     135.549ms         0.21%     135.549ms       5.157us         26282  \n",
      "                 aten::matmul         0.21%      28.557ms         0.48%      64.626ms      53.322us     117.395ms         0.18%     130.876ms     107.984us          1212  \n",
      "                   aten::sub_         0.22%      29.993ms         0.22%      29.993ms      23.001us     130.008ms         0.20%     130.008ms      99.699us          1304  \n",
      "      aten::cudnn_convolution         0.72%      96.171ms         0.83%     111.450ms     100.315us     117.111ms         0.18%     120.920ms     108.839us          1111  \n",
      "                     aten::ge         0.59%      79.356ms         1.02%     137.188ms      20.409us     104.999ms         0.16%     120.479ms      17.923us          6722  \n",
      "                 aten::gather         0.23%      30.286ms         0.27%      36.417ms      45.070us     114.776ms         0.18%     114.776ms     142.049us           808  \n",
      "                 aten::select         1.57%     210.972ms         1.84%     246.657ms       5.795us      99.439ms         0.15%      99.439ms       2.336us         42563  \n",
      "                 aten::cumsum         0.16%      21.384ms         0.85%     114.453ms      55.533us      48.050ms         0.07%      93.041ms      45.143us          2061  \n",
      "            aten::bitwise_and         0.66%      88.243ms         1.37%     184.329ms      21.747us      47.484ms         0.07%      80.488ms       9.496us          8476  \n",
      "                    aten::any         0.08%      11.276ms         0.10%      12.976ms      39.804us      72.492ms         0.11%      72.492ms     222.367us           326  \n",
      "    aten::adaptive_avg_pool2d         0.08%      11.130ms         0.17%      22.647ms      44.845us      61.086ms         0.10%      69.815ms     138.247us           505  \n",
      "                    aten::max         0.21%      27.872ms         0.26%      35.263ms      32.682us      64.941ms         0.10%      66.072ms      61.234us          1079  \n",
      "                 aten::clamp_         0.06%       8.240ms         0.10%      13.670ms      41.933us      58.657ms         0.09%      61.483ms     188.598us           326  \n",
      "            aten::bitwise_not         0.42%      55.774ms         0.86%     114.868ms      23.122us      33.417ms         0.05%      58.349ms      11.745us          4968  \n",
      "                aten::sigmoid         0.14%      18.652ms         0.21%      28.455ms      28.174us      55.433ms         0.09%      57.937ms      57.364us          1010  \n",
      "                 aten::__or__         0.05%       6.351ms         0.12%      15.997ms      49.071us      53.406ms         0.08%      57.048ms     174.993us           326  \n",
      "                     aten::le         0.11%      15.231ms         0.16%      21.784ms      33.411us      52.036ms         0.08%      53.457ms      81.989us           652  \n",
      "                aten::minimum         0.09%      11.415ms         0.09%      12.514ms      38.387us      45.954ms         0.07%      45.954ms     140.963us           326  \n",
      "               aten::scatter_         0.12%      15.548ms         0.12%      16.645ms      41.202us      45.674ms         0.07%      45.674ms     113.055us           404  \n",
      "                     aten::ne         0.13%      17.125ms         0.19%      24.872ms      30.783us      43.198ms         0.07%      45.300ms      56.064us           808  \n",
      "          aten::nonzero_numpy         0.01%       1.588ms         0.35%      46.331ms     229.361us       1.043ms         0.00%      44.289ms     219.254us           202  \n",
      "          aten::scalar_tensor         0.27%      36.060ms         0.70%      93.535ms      17.932us      18.133ms         0.03%      43.134ms       8.270us          5216  \n",
      "                  aten::fill_         0.51%      68.351ms         0.51%      68.351ms       7.261us      41.445ms         0.06%      41.445ms       4.403us          9413  \n",
      "                aten::_cumsum         0.33%      44.368ms         0.65%      87.832ms      42.616us      23.004ms         0.04%      41.168ms      19.975us          2061  \n",
      "                    aten::cat         0.12%      16.437ms         0.24%      32.808ms      54.138us      36.431ms         0.06%      39.628ms      65.392us           606  \n",
      "                   aten::sqrt         0.09%      12.233ms         0.14%      18.937ms      29.044us      36.746ms         0.06%      38.368ms      58.846us           652  \n",
      "                  aten::zero_         0.14%      18.233ms         0.30%      39.975ms      11.441us      15.544ms         0.02%      29.105ms       8.330us          3494  \n",
      "               aten::_s_where         0.42%      56.024ms         0.49%      65.928ms      19.431us      28.078ms         0.04%      28.078ms       8.275us          3393  \n",
      "                aten::softmax         0.03%       4.361ms         0.07%       9.829ms      48.659us      23.126ms         0.04%      25.545ms     126.461us           202  \n",
      "                   aten::mul_         0.07%       9.319ms         0.07%       9.319ms      28.587us      18.900ms         0.03%      18.900ms      57.975us           326  \n",
      "           aten::index_select         0.30%      40.563ms         0.35%      46.763ms      22.734us      17.781ms         0.03%      18.101ms       8.800us          2057  \n",
      "                 aten::repeat         0.14%      18.311ms         0.24%      32.546ms      76.219us       9.828ms         0.02%      16.318ms      38.215us           427  \n",
      "                     aten::mm         0.21%      28.348ms         0.25%      33.871ms      27.946us      13.143ms         0.02%      13.143ms      10.844us          1212  \n",
      "                   aten::topk         0.07%       9.674ms         0.07%       9.674ms      14.837us      11.620ms         0.02%      11.620ms      17.822us           652  \n",
      "               aten::uniform_         0.06%       7.708ms         0.06%       7.708ms      11.823us      10.134ms         0.02%      10.134ms      15.543us           652  \n",
      "             aten::threshold_         0.11%      14.427ms         0.11%      14.427ms       9.523us       9.241ms         0.01%       9.241ms       6.100us          1515  \n",
      "                  aten::clone         0.04%       5.812ms         0.07%       9.516ms      47.110us       8.308ms         0.01%       9.172ms      45.406us           202  \n",
      "                   aten::ones         0.04%       4.900ms         0.05%       7.138ms      35.336us       8.009ms         0.01%       8.874ms      43.930us           202  \n",
      "                   aten::mean         0.07%       9.573ms         0.09%      11.517ms      22.806us       8.729ms         0.01%       8.729ms      17.284us           505  \n",
      "              aten::ones_like         0.03%       4.398ms         0.06%       7.388ms      36.576us       6.569ms         0.01%       7.438ms      36.824us           202  \n",
      "                aten::argsort         0.03%       4.588ms         0.08%      11.059ms      54.749us       3.803ms         0.01%       7.206ms      35.672us           202  \n",
      "                    aten::min         0.09%      12.072ms         0.12%      16.184ms      24.821us       5.797ms         0.01%       6.916ms      10.607us           652  \n",
      "             aten::bitwise_or         0.06%       7.667ms         0.12%      15.942ms      24.452us       3.641ms         0.01%       6.175ms       9.471us           652  \n",
      "                  aten::alias         0.09%      11.457ms         0.09%      11.457ms       7.973us       6.152ms         0.01%       6.152ms       4.281us          1437  \n",
      "                   aten::log_         0.03%       3.682ms         0.07%       8.745ms      13.413us       2.252ms         0.00%       5.286ms       8.107us           652  \n",
      "                    aten::neg         0.03%       3.740ms         0.04%       5.642ms      27.932us       4.132ms         0.01%       4.577ms      22.656us           202  \n",
      "                   aten::div_         0.04%       4.812ms         0.04%       4.812ms       7.381us       3.862ms         0.01%       3.862ms       5.924us           652  \n",
      "           aten::masked_fill_         0.03%       3.583ms         0.03%       3.583ms      35.476us       3.850ms         0.01%       3.850ms      38.116us           101  \n",
      "                aten::flatten         0.04%       5.091ms         0.04%       5.091ms       2.475us       3.846ms         0.01%       3.846ms       1.870us          2057  \n",
      "               aten::linspace         0.02%       2.922ms         0.03%       3.998ms      19.793us       3.273ms         0.01%       3.790ms      18.765us           202  \n",
      "                   aten::sort         0.05%       6.471ms         0.05%       6.471ms      32.036us       3.403ms         0.01%       3.403ms      16.844us           202  \n",
      "                   aten::_cat         0.09%      11.636ms         0.12%      16.370ms      27.014us       3.196ms         0.00%       3.196ms       5.275us           606  \n",
      "              aten::expand_as         0.03%       3.505ms         0.07%       8.973ms       7.913us       3.099ms         0.00%       3.099ms       2.732us          1134  \n",
      "                    aten::log         0.04%       5.063ms         0.04%       5.063ms       7.765us       3.034ms         0.00%       3.034ms       4.654us           652  \n",
      "                  aten::clamp         0.02%       2.156ms         0.04%       5.430ms      16.658us       1.111ms         0.00%       2.826ms       8.667us           326  \n",
      "                 aten::unbind         0.02%       3.034ms         0.04%       5.061ms      25.054us       1.744ms         0.00%       2.441ms      12.085us           202  \n",
      "               aten::_softmax         0.03%       3.821ms         0.04%       5.468ms      27.068us       2.085ms         0.00%       2.419ms      11.978us           202  \n",
      "              aten::clamp_min         0.02%       3.275ms         0.02%       3.275ms      10.045us       1.715ms         0.00%       1.715ms       5.260us           326  \n",
      "                 aten::unfold         0.02%       2.309ms         0.02%       2.951ms       6.911us     884.955us         0.00%     884.955us       2.072us           427  \n",
      "                  aten::slice         1.01%     134.951ms         1.27%     169.793ms       4.197us       0.000us         0.00%       0.000us       0.000us         40456  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.410s\n",
      "CUDA time total: 64.221s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_level_events_only = False\n",
    "if torch.device == torch.device('cpu'):\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", top_level_events_only=top_level_events_only))\n",
    "else:\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", top_level_events_only=top_level_events_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-smile",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = \n",
    "\n",
    "moves = torch.zeros((test.n_envs, test.n_geese), dtype=torch.int64, device=device) + 1\n",
    "update_mask = torch.ones((test.n_envs,), dtype=torch.bool, device=device)\n",
    "update_mask[-1] = False\n",
    "\n",
    "profiling_enabled = False\n",
    "with torch.autograd.profiler.profile(enabled=profiling_enabled, use_cuda=True) as prof:\n",
    "    test.reset()\n",
    "    for i in tqdm.trange(40):\n",
    "        test.step(moves, update_mask)\n",
    "print(test.render_env(0, include_info=True))\n",
    "if profiling_enabled:\n",
    "    prof.export_chrome_trace(f'trace_{str(device).split(\":\")[0]}_{test.n_envs}_envs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.render_env(-1, include_info=True))\n",
    "test.obs[-1][test.obs_channel_idxs['steps_since_starvation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_events_only = True\n",
    "if torch.device == torch.device('cpu'):\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n",
    "else:\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", top_level_events_only=top_level_events_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_idx = 1\n",
    "test.geese_tensor[env_idx], test.lengths[env_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.geese[sc].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(8).view(2, 4) % torch.tensor([2, 3]).view(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test.geese[torch.arange(test.n_envs)[test.dones], :, 0] = 1\n",
    "test.geese.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_rows = 7\n",
    "n_cols = 11\n",
    "\n",
    "head_locs = torch.multinomial(\n",
    "    torch.ones((3, n_rows * n_cols)),\n",
    "    4\n",
    ")\n",
    "loc_to_row_column = torch.tensor(\n",
    "    [row_col(i, n_cols) for i in range(n_rows * n_cols)],\n",
    ").view(n_rows * n_cols, 2)\n",
    "loc_to_row_column[head_locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle_environments\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Action, Configuration, Observation\n",
    "\n",
    "from hungry_geese.utils import read_json\n",
    "from hungry_geese.env.lightweight_env import make_from_state\n",
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_from_state(Observation(env.steps[-2][0]['observation']), [Action.NORTH] * 4).done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_dict = read_json('/home/isaiah/Downloads/19998794.json')\n",
    "env = kaggle_environments.make(\n",
    "    'hungry_geese',\n",
    "    configuration=replay_dict['configuration'],\n",
    "    steps=replay_dict['steps'],\n",
    "    info=replay_dict['info']\n",
    ")\n",
    "\n",
    "conf = env.configuration\n",
    "env.steps[119][0]['observation']['step'] = 0\n",
    "\n",
    "main.AGENT = None\n",
    "main.call_agent(env.steps[119][0]['observation'], conf)\n",
    "main.call_agent(env.steps[120][0]['observation'], conf)\n",
    "\"\"\"\n",
    "obs = Observation(Observation(env.steps[120][0]['observation']))\n",
    "main.AGENT.preprocess(obs, conf)\n",
    "light_env = make_from_state(obs, main.AGENT.last_actions)\n",
    "main.AGENT.search_tree.run_batch_mcts(light_env.lightweight_clone(), 1, n_iter=2)\n",
    "out = main.AGENT.search_tree.expand(light_env)\n",
    "out\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_env.step_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(light_env.render_ansi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='ipython', height=700, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_geese = 4\n",
    "values = torch.arange(8).view(2,4).to(torch.float)\n",
    "still_alive = torch.ones_like(values).to(torch.bool)\n",
    "still_alive[0,0:2] = False\n",
    "\n",
    "\n",
    "values.masked_fill_(~still_alive, float('-inf'))\n",
    "win_probs = torch.softmax(values, dim=-1)\n",
    "print(win_probs)\n",
    "remaining_rewards = torch.linspace(0., 1., n_geese, dtype=torch.float)\n",
    "remaining_rewards_min = remaining_rewards[-still_alive.sum(dim=-1)].unsqueeze(-1)\n",
    "remaining_rewards_var = 1. - remaining_rewards_min\n",
    "values = remaining_rewards_min + win_probs * remaining_rewards_var\n",
    "# TODO: This is a hacky solution - there should be a more elegant way to do this for any n_geese_remaining?\n",
    "print(values)\n",
    "values = torch.where(\n",
    "    still_alive.sum(dim=-1, keepdim=True) == 4,\n",
    "    values * 2.,\n",
    "    values\n",
    ")\n",
    "values = torch.where(\n",
    "    still_alive.sum(dim=-1, keepdim=True) == 3,\n",
    "    values * 1.2,\n",
    "    values\n",
    ")\n",
    "\n",
    "print(values)\n",
    "max_vals = values.max(dim=-1, keepdim=True)[0]\n",
    "values = torch.where(\n",
    "    max_vals > 1.,\n",
    "    torch.where(\n",
    "        values == max_vals,\n",
    "        values - (max_vals - 1.),\n",
    "        values + (max_vals - 1.) / still_alive.sum(dim=-1, keepdim=True)\n",
    "    ),\n",
    "    values\n",
    ")\n",
    "print(values)\n",
    "values * 2. - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "obs_type = ge.ObsType.COMBINED_GRADIENT_OBS\n",
    "n_channels = 128\n",
    "activation = nn.ReLU\n",
    "normalize = False\n",
    "model_kwargs = dict(\n",
    "    block_class=models.BasicConvolutionalBlock,\n",
    "    block_kwargs=[\n",
    "        dict(\n",
    "            in_channels=obs_type.get_obs_spec()[-3],\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=normalize\n",
    "        ),\n",
    "    ],\n",
    "    squeeze_excitation=True,\n",
    "    cross_normalize_value=True,\n",
    "    # **ge.RewardType.RANK_ON_DEATH.get_recommended_value_activation_scale_shift_dict()\n",
    ")\n",
    "model = models.FullConvActorCriticNetwork(**model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import copy\n",
    "from kaggle_environments import make as kaggle_make\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Action, Configuration, Observation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy import special, stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hungry_geese import models\n",
    "from hungry_geese.utils import ActionMasking\n",
    "from hungry_geese.env import goose_env as ge\n",
    "from hungry_geese.env.lightweight_env import LightweightEnv, make_from_state\n",
    "from hungry_geese.mcts.basic_mcts import BasicMCTS\n",
    "from hungry_geese.training.alphagoose.alphagoose_data import AlphaGoosePretrainDataset, ToTensor\n",
    "from hungry_geese.utils import read_json\n",
    "\n",
    "%matplotlib inline\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_type = ge.ObsType.COMBINED_GRADIENT_OBS\n",
    "n_channels = 64\n",
    "activation = nn.ReLU\n",
    "model_kwargs = dict(\n",
    "    block_class=models.BasicConvolutionalBlock,\n",
    "    block_kwargs=[\n",
    "        dict(\n",
    "            in_channels=obs_type.get_obs_spec()[-3],\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=False\n",
    "        ),\n",
    "        dict(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            kernel_size=3,\n",
    "            activation=activation,\n",
    "            normalize=False\n",
    "        ),\n",
    "    ],\n",
    "    squeeze_excitation=True,\n",
    "    cross_normalize_value=True,\n",
    "    # **ge.RewardType.RANK_ON_DEATH.get_recommended_value_activation_scale_shift_dict()\n",
    ")\n",
    "model = models.FullConvActorCriticNetwork(**model_kwargs)\n",
    "model.to(device=DEVICE)\n",
    "\n",
    "run_dir = Path('runs/supervised_pretraining/active/supervised_pretraining_combined_gradient_obs_rank_on_death_none_5_blocks_64_dims_v1/')\n",
    "with open(run_dir / '0070/cp.txt', 'r') as f:\n",
    "    serialized_string = f.readline()[2:-1].encode()\n",
    "state_dict_bytes = base64.b64decode(serialized_string)\n",
    "loaded_state_dicts = pickle.loads(state_dict_bytes)\n",
    "model.load_state_dict(loaded_state_dicts)\n",
    "\n",
    "def action_mask_func(state):\n",
    "    return ActionMasking.LETHAL.get_action_mask(state)\n",
    "    \n",
    "def terminal_value_func(state):\n",
    "    agent_rankings = stats.rankdata([agent['reward'] for agent in state], method='average') - 1.\n",
    "    ranks_rescaled = 2. * agent_rankings / (len(state) - 1.) - 1.\n",
    "    return ranks_rescaled\n",
    "    \n",
    "def actor_critic_func(state):\n",
    "    geese = state[0]['observation']['geese']\n",
    "    n_geese = len(geese)\n",
    "    \n",
    "    obs = ge.create_obs_tensor(state, obs_type)\n",
    "    head_locs = [goose[0] if len(goose) > 0 else -1 for goose in geese]\n",
    "    still_alive = [agent['status'] == 'ACTIVE' for agent in state]\n",
    "    with torch.no_grad():\n",
    "        logits, values = model(torch.from_numpy(obs),\n",
    "                               torch.tensor(head_locs).unsqueeze(0),\n",
    "                               torch.tensor(still_alive).unsqueeze(0))\n",
    "    \n",
    "    # Score the dead geese\n",
    "    dead_geese_mask = np.array([len(goose) for goose in geese]) == 0\n",
    "    agent_rankings = stats.rankdata([agent['reward'] for agent in state], method='average') - 1.\n",
    "    agent_rankings_rescaled = 2. * agent_rankings / (n_geese - 1.) - 1.\n",
    "    \n",
    "    logits = F.softmax(logits, -1)\n",
    "    final_values = np.where(\n",
    "        dead_geese_mask,\n",
    "        agent_rankings_rescaled,\n",
    "        values.squeeze(0).numpy()\n",
    "    )\n",
    "    \n",
    "    # Logits should be of shape (4, 4)\n",
    "    # Values should be of shape (4, 1)\n",
    "    return logits.squeeze(0).numpy().astype(np.float), final_values[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_dir / 'train_episodes.txt', 'r') as f:\n",
    "    train_episodes = [replay_name.rstrip() for replay_name in f.readlines()]\n",
    "    train_episodes = set(train_episodes)\n",
    "with open(run_dir / 'test_episodes.txt', 'r') as f:\n",
    "    test_episodes = [replay_name.rstrip() for replay_name in f.readlines()]\n",
    "    test_episodes = set(test_episodes)\n",
    "\n",
    "dataset_loc = Path('/home/isaiah/data/alphagoose_data_1000/')\n",
    "train_dataset = AlphaGoosePretrainDataset(\n",
    "    dataset_loc,\n",
    "    ge.ObsType.COMBINED_GRADIENT_OBS,\n",
    "    transform=ToTensor(),\n",
    "    include_episode=lambda x: x.stem in train_episodes\n",
    ")\n",
    "test_dataset = AlphaGoosePretrainDataset(\n",
    "    dataset_loc,\n",
    "    ge.ObsType.COMBINED_GRADIENT_OBS,\n",
    "    transform=ToTensor(),\n",
    "    include_episode=lambda x: x.stem in test_episodes\n",
    ")\n",
    "dataloader_kwargs = dict(\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    "    pin_memory=True\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, **dataloader_kwargs)\n",
    "test_dataloader = DataLoader(test_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(\n",
    "        state,\n",
    "        action,\n",
    "        result,\n",
    "        head_locs,\n",
    "        still_alive,\n",
    "        reduction='mean',\n",
    "        get_preds=False\n",
    "):\n",
    "    with amp.autocast():\n",
    "        logits, value = model(state, head_locs, still_alive)\n",
    "\n",
    "        logits_masked = logits.view(-1, 4)[still_alive.view(-1, 1).expand(-1, 4)].view(-1, 4)\n",
    "        action_masked = action.view(-1)[still_alive.view(-1)]\n",
    "        policy_loss = F.cross_entropy(logits_masked, action_masked, reduction=reduction)\n",
    "\n",
    "        value_masked = value.view(-1)[still_alive.view(-1)]\n",
    "        result_masked = result.view(-1)[still_alive.view(-1)]\n",
    "        value_loss = F.mse_loss(value_masked, result_masked, reduction=reduction)\n",
    "\n",
    "        probs_masked = F.softmax(logits_masked, dim=-1)\n",
    "        entropy_loss = torch.sum(probs_masked * torch.log(probs_masked), dim=-1)\n",
    "        if reduction == 'none':\n",
    "            pass\n",
    "        elif reduction == 'mean':\n",
    "            entropy_loss = entropy_loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            entropy_loss = entropy_loss.sum()\n",
    "        else:\n",
    "            raise ValueError(f'Unrecognized reduction: {reduction}')\n",
    "\n",
    "        combined_loss = policy_loss + value_loss + 0.05 * entropy_loss\n",
    "\n",
    "    if get_preds:\n",
    "        return policy_loss, value_loss, entropy_loss, combined_loss, logits_masked.argmax(dim=-1)\n",
    "    else:\n",
    "        return policy_loss, value_loss, entropy_loss, combined_loss\n",
    "\n",
    "test_metrics = {\n",
    "    'policy_loss': 0.,\n",
    "    'value_loss': 0.,\n",
    "    'entropy_loss': 0.,\n",
    "    'combined_loss': 0.,\n",
    "    'policy_accuracy': 0.\n",
    "}\n",
    "n_test_samples = 0.\n",
    "with torch.no_grad():\n",
    "    for test_tuple in tqdm(test_dataloader):\n",
    "        test_tuple = [t.to(device=DEVICE) for t in test_tuple]\n",
    "        state, action, result, head_locs, still_alive = test_tuple\n",
    "        policy_loss, value_loss, entropy_loss, combined_loss, preds = compute_losses(\n",
    "            *test_tuple,\n",
    "            reduction='mean',\n",
    "            get_preds=True\n",
    "        )\n",
    "        action_masked = action.view(-1)[still_alive.view(-1)]\n",
    "\n",
    "        test_metrics['policy_loss'] += policy_loss.detach().cpu().item()\n",
    "        test_metrics['value_loss'] += value_loss.detach().cpu().item()\n",
    "        test_metrics['entropy_loss'] += entropy_loss.detach().cpu().item()\n",
    "        test_metrics['combined_loss'] += combined_loss.detach().cpu().item()\n",
    "        test_metrics['policy_accuracy'] += preds.eq(action_masked).sum().cpu().item()\n",
    "        n_test_samples += still_alive.sum().cpu().item()\n",
    "\n",
    "    for key, metric in test_metrics.items():\n",
    "        test_metrics[key] = metric / n_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train metrics\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train metrics\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train metrics\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = kaggle_make('hungry_geese', debug=True)\n",
    "#env.reset(num_agents=4);\n",
    "replay_dict = read_json('/home/isaiah/Downloads/19528131.json')\n",
    "env = kaggle_make(\n",
    "    'hungry_geese',\n",
    "    configuration=replay_dict['configuration'],\n",
    "    steps=replay_dict['steps'],\n",
    "    info=replay_dict['info']\n",
    ")\n",
    "env.render(mode='ipython', height=700, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tree = BasicMCTS(\n",
    "    action_mask_func=action_mask_func,\n",
    "    actor_critic_func=actor_critic_func,\n",
    "    terminal_value_func=terminal_value_func,\n",
    "    c_puct = 1.,\n",
    "    include_food=False\n",
    ")\n",
    "step_idx = 120\n",
    "state = env.steps[step_idx][0]\n",
    "state.update(env.steps[step_idx][0]['observation'])\n",
    "obs = Observation(state)\n",
    "light_env = make_from_state(\n",
    "    obs,\n",
    "    [Action[agent['action']] for agent in env.steps[-1]],\n",
    "    env.configuration\n",
    ")\n",
    "print(light_env.render_ansi())\n",
    "\n",
    "root_node = search_tree.run_mcts(\n",
    "    env=light_env,\n",
    "    n_iter=10000,\n",
    "    max_time=0.5\n",
    ")\n",
    "print(root_node.initial_policies)\n",
    "print(root_node.initial_values)\n",
    "print(root_node.q_vals)\n",
    "print(root_node.n_visits / root_node.n_visits.sum(axis=1, keepdims=True))\n",
    "print(root_node.n_visits.sum(axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-morris",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_tree = BasicMCTS(\n",
    "    action_mask_func=action_mask_func,\n",
    "    actor_critic_func=actor_critic_func,\n",
    "    terminal_value_func=terminal_value_func,\n",
    "    c_puct = np.sqrt(2.),\n",
    "    include_food=False\n",
    ")\n",
    "while not env.done:\n",
    "    state = env.steps[-1][0]\n",
    "    state.update(env.steps[-1][0]['observation'])\n",
    "    obs = Observation(state)\n",
    "    light_env = make_from_state(\n",
    "        obs,\n",
    "        [Action[agent['action']] for agent in env.steps[-1]],\n",
    "        env.configuration\n",
    "    )\n",
    "    print(light_env.render_ansi())\n",
    "    \n",
    "    csr = light_env.canonical_string_repr(include_food=search_tree.include_food)\n",
    "    for key in list(search_tree.nodes.keys()):\n",
    "        if key.startswith(f'S: {obs.step - 1}') or (key.startswith(f'S: {obs.step}') and key != csr):\n",
    "            del search_tree.nodes[key]\n",
    "    root_node = search_tree.run_mcts(\n",
    "        env=light_env,\n",
    "        n_iter=10000,\n",
    "        max_time=0.3\n",
    "    )\n",
    "    print(root_node.n_visits)\n",
    "    print(int(root_node.n_visits.sum() / 4))\n",
    "    actions = root_node.get_improved_actions(0.)\n",
    "    env.step([tuple(Action)[a].name for a in actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic_func(env.steps[-12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(mode='ipython', height=700, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(root_node.n_visits)\n",
    "print(int(root_node.n_visits.sum() / 4))\n",
    "root_node.q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "for s, a, r, hl, alive in ag_dataloader:\n",
    "    square = np.ceil(np.sqrt(s.shape[1])).astype(np.int)\n",
    "    fig, axs = plt.subplots(square, square, figsize=(20,10))\n",
    "    ix = 0\n",
    "    for i in range(square):\n",
    "        for j in range(square):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            eps = 1e-2\n",
    "            if ix < s.shape[1]:\n",
    "                pcm = ax.imshow(s[0, ix, :, :] + eps, norm=colors.LogNorm(vmin=eps, vmax=1.), cmap='gray')\n",
    "                ix += 1\n",
    "    fig.colorbar(pcm, ax = axs)\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
