{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010484,
     "end_time": "2021-02-10T14:56:52.720300",
     "exception": false,
     "start_time": "2021-02-10T14:56:52.709816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simulations Episode Scraper Match Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008843,
     "end_time": "2021-02-10T14:56:52.739107",
     "exception": false,
     "start_time": "2021-02-10T14:56:52.730264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From Kaggle user robga: https://www.kaggle.com/robga/simulations-episode-scraper-match-downloader\n",
    "\n",
    "This notebook downloads episodes using Kaggle's GetEpisodeReplay API and the [Meta Kaggle](https://www.kaggle.com/kaggle/meta-kaggle) dataset.\n",
    "\n",
    "Meta Kaggle is refreshed daily, and sometimes fails a daily refresh. That's OK, Goose keeps well for 24hr.\n",
    "\n",
    "Why download replays?\n",
    "- Train your ML/RL model\n",
    "- Inspect the performance of yours and others agents\n",
    "- To add to your ever growing json collection \n",
    "\n",
    "Only one scraping strategy is implemented: For each top scoring submission, download all missing matches, move on to next submission.\n",
    "\n",
    "Other scraping strategies can be implemented, but not here. Like download max X matches per submission or per team per day, or ignore certain teams or ignore where some scores < X, or only download some teams.\n",
    "\n",
    "Please let me know of any bugs. It's new, and my goose may be cooked.\n",
    "\n",
    "Todo:\n",
    "- Add teamid's once meta kaggle add them (a few days away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-10T14:56:52.763232Z",
     "iopub.status.busy": "2021-02-10T14:56:52.762225Z",
     "iopub.status.idle": "2021-02-10T14:56:52.765162Z",
     "shell.execute_reply": "2021-02-10T14:56:52.764514Z"
    },
    "papermill": {
     "duration": 0.017196,
     "end_time": "2021-02-10T14:56:52.765344",
     "exception": false,
     "start_time": "2021-02-10T14:56:52.748148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import collections\n",
    "\n",
    "global num_api_calls_today\n",
    "num_api_calls_today = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-10T14:56:52.786896Z",
     "iopub.status.busy": "2021-02-10T14:56:52.786201Z",
     "iopub.status.idle": "2021-02-10T14:56:52.790237Z",
     "shell.execute_reply": "2021-02-10T14:56:52.790825Z"
    },
    "papermill": {
     "duration": 0.016481,
     "end_time": "2021-02-10T14:56:52.791002",
     "exception": false,
     "start_time": "2021-02-10T14:56:52.774521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You should configure these to your needs. Choose one of ...\n",
    "# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\n",
    "COMP = 'hungry-geese'\n",
    "MAX_CALLS_PER_DAY = 3500\n",
    "LOWEST_SCORE_THRESH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-10T14:56:52.813265Z",
     "iopub.status.busy": "2021-02-10T14:56:52.812605Z",
     "iopub.status.idle": "2021-02-10T14:56:52.817427Z",
     "shell.execute_reply": "2021-02-10T14:56:52.818012Z"
    },
    "papermill": {
     "duration": 0.017634,
     "end_time": "2021-02-10T14:56:52.818196",
     "exception": false,
     "start_time": "2021-02-10T14:56:52.800562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "META = \"episode_scraping/metadata/\"\n",
    "MATCH_DIR = 'episode_scraping/episodes/'\n",
    "INFO_DIR = 'episode_scraping/infos/'\n",
    "base_url = \"https://www.kaggle.com/requests/EpisodeService/\"\n",
    "get_url = base_url + \"GetEpisodeReplay\"\n",
    "BUFFER = 1\n",
    "COMPETITIONS = {\n",
    "    'hungry-geese': 25401,\n",
    "    'rock-paper-scissors': 22838,\n",
    "    'santa-2020': 24539,\n",
    "    'halite': 18011,\n",
    "    'google-football': 21723\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-10T14:56:52.840206Z",
     "iopub.status.busy": "2021-02-10T14:56:52.839525Z",
     "iopub.status.idle": "2021-02-10T14:58:53.315656Z",
     "shell.execute_reply": "2021-02-10T14:58:53.316208Z"
    },
    "papermill": {
     "duration": 120.489076,
     "end_time": "2021-02-10T14:58:53.316601",
     "exception": false,
     "start_time": "2021-02-10T14:56:52.827525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes.csv: 19798858 rows before filtering.\n",
      "EpisodeAgents.csv: 44111716 rows before filtering.\n",
      "Episodes.csv: 433431 rows after filtering for hungry-geese.\n",
      "EpisodeAgents.csv: 1733724 rows after filtering for hungry-geese.\n"
     ]
    }
   ],
   "source": [
    "# Load Episodes\n",
    "episodes_df = pd.read_csv(META + \"Episodes.csv\")\n",
    "\n",
    "# Load EpisodeAgents\n",
    "epagents_df = pd.read_csv(META + \"EpisodeAgents.csv\")\n",
    "\n",
    "print(f'Episodes.csv: {len(episodes_df)} rows before filtering.')\n",
    "print(f'EpisodeAgents.csv: {len(epagents_df)} rows before filtering.')\n",
    "\n",
    "episodes_df = episodes_df[episodes_df.CompetitionId == COMPETITIONS[COMP]] \n",
    "epagents_df = epagents_df[epagents_df.EpisodeId.isin(episodes_df.Id)]\n",
    "\n",
    "print(f'Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.')\n",
    "print(f'EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.')\n",
    "\n",
    "# Prepare dataframes\n",
    "episodes_df = episodes_df.set_index(['Id'])\n",
    "episodes_df['CreateTime'] = pd.to_datetime(episodes_df['CreateTime'])\n",
    "episodes_df['EndTime'] = pd.to_datetime(episodes_df['EndTime'])\n",
    "\n",
    "epagents_df.fillna(0, inplace=True)\n",
    "epagents_df = epagents_df.sort_values(by=['Id'], ascending=False)\n",
    "\n",
    "latest_scores_df = epagents_df.loc[epagents_df.groupby('SubmissionId').EpisodeId.idxmax(),:].sort_values(by=['UpdatedScore'])\n",
    "latest_scores_df['LatestScore'] = latest_scores_df.UpdatedScore\n",
    "latest_scores_df = latest_scores_df[['SubmissionId', 'LatestScore']]\n",
    "epagents_df = epagents_df.merge(latest_scores_df, left_on='SubmissionId', right_on='SubmissionId', how='outer').sort_values(by=['LatestScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48738 episodes with all agent scores over 1000\n",
      "8949 of these 48738 episodes not yet saved\n",
      "Total of 83997 games in existing library\n"
     ]
    }
   ],
   "source": [
    "# Get episodes with all agent scores > a given threshold\n",
    "episode_min_scores = epagents_df.groupby('EpisodeId').LatestScore.min()\n",
    "ep_to_score = episode_min_scores[episode_min_scores >= LOWEST_SCORE_THRESH].to_dict()\n",
    "print(f'{len(ep_to_score)} episodes with all agent scores over {LOWEST_SCORE_THRESH}')\n",
    "\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n",
    "    all_files.extend(files)\n",
    "seen_episodes = [int(f.split('.')[0]) for f in all_files \n",
    "                 if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\n",
    "remaining = np.setdiff1d([ep for ep in ep_to_score.keys()], seen_episodes)\n",
    "print(f'{len(remaining)} of these {len(ep_to_score)} episodes not yet saved')\n",
    "print('Total of {} games in existing library'.format(len(seen_episodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-10T14:58:58.955827Z",
     "iopub.status.busy": "2021-02-10T14:58:58.955091Z",
     "iopub.status.idle": "2021-02-10T14:58:58.957904Z",
     "shell.execute_reply": "2021-02-10T14:58:58.958411Z"
    },
    "papermill": {
     "duration": 0.025925,
     "end_time": "2021-02-10T14:58:58.958598",
     "exception": false,
     "start_time": "2021-02-10T14:58:58.932673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_info_json(epid):\n",
    "    \n",
    "    create_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item()/1e9)\n",
    "    end_seconds = int((episodes_df[episodes_df.index == epid]['CreateTime'].values[0]).item()/1e9)\n",
    "\n",
    "    agents = []\n",
    "    for index, row in epagents_df[epagents_df['EpisodeId'] == epid].sort_values(by=['Index']).iterrows():\n",
    "        agent = {\n",
    "            \"id\": int(row[\"Id\"]),\n",
    "            \"state\": int(row[\"State\"]),\n",
    "            \"submissionId\": int(row['SubmissionId']),\n",
    "            \"reward\": int(row['Reward']),\n",
    "            \"index\": int(row['Index']),\n",
    "            \"initialScore\": float(row['InitialScore']),\n",
    "            \"initialConfidence\": float(row['InitialConfidence']),\n",
    "            \"updatedScore\": float(row['UpdatedScore']),\n",
    "            \"updatedConfidence\": float(row['UpdatedConfidence']),\n",
    "            \"teamId\": int(99999)\n",
    "        }\n",
    "        agents.append(agent)\n",
    "\n",
    "    info = {\n",
    "        \"id\": int(epid),\n",
    "        \"competitionId\": int(COMPETITIONS[COMP]),\n",
    "        \"createTime\": {\n",
    "            \"seconds\": int(create_seconds)\n",
    "        },\n",
    "        \"endTime\": {\n",
    "            \"seconds\": int(end_seconds)\n",
    "        },\n",
    "        \"agents\": agents\n",
    "    }\n",
    "\n",
    "    return info\n",
    "\n",
    "def saveEpisode(epid):\n",
    "    # request\n",
    "    re = requests.post(get_url, json = {\"EpisodeId\": int(epid)})\n",
    "        \n",
    "    # save replay\n",
    "    replay = re.json()['result']['replay']\n",
    "    with open(MATCH_DIR + '{}.json'.format(epid), 'w') as f:\n",
    "        f.write(replay)\n",
    "\n",
    "    # save match info\n",
    "    info = create_info_json(epid)\n",
    "    with open(INFO_DIR +  '{}_info.json'.format(epid), 'w') as f:\n",
    "        json.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-02-10T14:58:59.034547Z",
     "iopub.status.busy": "2021-02-10T14:58:59.033830Z",
     "iopub.status.idle": "2021-02-10T15:00:41.353097Z",
     "shell.execute_reply": "2021-02-10T15:00:41.353911Z"
    },
    "papermill": {
     "duration": 102.344662,
     "end_time": "2021-02-10T15:00:41.354286",
     "exception": false,
     "start_time": "2021-02-10T14:58:59.009624",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: saved episode #19901347\n",
      "2: saved episode #19559919\n",
      "3: saved episode #19724146\n",
      "4: saved episode #19775709\n",
      "5: saved episode #19781282\n",
      "6: saved episode #19796499\n",
      "7: saved episode #19813971\n",
      "8: saved episode #19822125\n",
      "9: saved episode #19837277\n",
      "10: saved episode #19848215\n",
      "11: saved episode #19868360\n",
      "12: saved episode #19876797\n",
      "13: saved episode #19890519\n",
      "14: saved episode #19591998\n",
      "15: saved episode #19758507\n",
      "16: saved episode #19821732\n",
      "17: saved episode #19822360\n",
      "18: saved episode #19827704\n",
      "19: saved episode #19836549\n",
      "20: saved episode #19853901\n",
      "21: saved episode #19854875\n",
      "22: saved episode #19857749\n",
      "23: saved episode #19882332\n",
      "24: saved episode #19893835\n",
      "25: saved episode #17486045\n",
      "26: saved episode #19056461\n",
      "27: saved episode #19228154\n",
      "28: saved episode #19446886\n",
      "29: saved episode #19447426\n"
     ]
    }
   ],
   "source": [
    "r = BUFFER;\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "se=0\n",
    "for epid, value in sorted(ep_to_score.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    if num_api_calls_today <= MAX_CALLS_PER_DAY:\n",
    "        if epid not in seen_episodes and num_api_calls_today < MAX_CALLS_PER_DAY:\n",
    "            try:\n",
    "                saveEpisode(epid); \n",
    "            except requests.exceptions.ConnectionError:\n",
    "                pass\n",
    "            r+=1;\n",
    "            se+=1\n",
    "            try:\n",
    "                size = os.path.getsize(MATCH_DIR+'{}.json'.format(epid)) / 1e6\n",
    "                print(f'{num_api_calls_today+1}: saved episode #{epid}')\n",
    "                seen_episodes.append(epid)\n",
    "                num_api_calls_today+=1\n",
    "            except:\n",
    "                print('  file {}.json did not seem to save'.format(epid))\n",
    "                se -= 1\n",
    "            if r > (datetime.datetime.now() - start_time).seconds:\n",
    "                time.sleep( r - (datetime.datetime.now() - start_time).seconds)\n",
    "        if num_api_calls_today >= (min(3600, MAX_CALLS_PER_DAY)):\n",
    "            break\n",
    "print('')\n",
    "print(f'Episodes saved: {se}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042895,
     "end_time": "2021-02-10T15:00:41.441307",
     "exception": false,
     "start_time": "2021-02-10T15:00:41.398412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Deprecated - filter episodes by submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-02-10T14:58:58.444486Z",
     "iopub.status.busy": "2021-02-10T14:58:58.443848Z",
     "iopub.status.idle": "2021-02-10T14:58:58.488699Z",
     "shell.execute_reply": "2021-02-10T14:58:58.487828Z"
    },
    "papermill": {
     "duration": 0.058221,
     "end_time": "2021-02-10T14:58:58.488913",
     "exception": false,
     "start_time": "2021-02-10T14:58:58.430692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 submissions with score over 1050\n",
      "31948 episodes for these 512 submissions\n",
      "10224 of these 31948 episodes not yet saved\n",
      "Total of 23850 games in existing library\n"
     ]
    }
   ],
   "source": [
    "# Get top scoring submissions\n",
    "max_df = (epagents_df.sort_values(by=['EpisodeId'], ascending=False).groupby('SubmissionId').head(1).drop_duplicates().reset_index(drop=True))\n",
    "max_df = max_df[max_df.UpdatedScore>=LOWEST_SCORE_THRESH]\n",
    "max_df = pd.merge(left=episodes_df, right=max_df, left_on='Id', right_on='EpisodeId')\n",
    "sub_to_score_top = pd.Series(max_df.UpdatedScore.values,index=max_df.SubmissionId).to_dict()\n",
    "print(f'{len(sub_to_score_top)} submissions with score over {LOWEST_SCORE_THRESH}')\n",
    "\n",
    "# Get episodes for these submissions\n",
    "sub_to_episodes = collections.defaultdict(list)\n",
    "for key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    eps = sorted(epagents_df[epagents_df['SubmissionId'].isin([key])]['EpisodeId'].values,reverse=True)\n",
    "    sub_to_episodes[key] = eps\n",
    "candidates = len(set([item for sublist in sub_to_episodes.values() for item in sublist]))\n",
    "print(f'{candidates} episodes for these {len(sub_to_score_top)} submissions')\n",
    "\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n",
    "    all_files.extend(files)\n",
    "seen_episodes = [int(f.split('.')[0]) for f in all_files \n",
    "                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\n",
    "remaining = np.setdiff1d([item for sublist in sub_to_episodes.values() for item in sublist], seen_episodes)\n",
    "print(f'{len(remaining)} of these {candidates} episodes not yet saved')\n",
    "print('Total of {} games in existing library'.format(len(seen_episodes)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 235.299673,
   "end_time": "2021-02-10T15:00:42.196308",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-10T14:56:46.896635",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
